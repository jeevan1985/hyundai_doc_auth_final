# core_engine/image_similarity_system/workflow.py
"""
TIF-focused workflows for document similarity:
- build_index_from_tif_folder_workflow: OCR pages -> extract photos -> save crops -> build FAISS index
- execute_tif_batch_search_workflow: OCR pages -> extract photos -> search -> aggregate to parent TIF docs

All non-TIF, generic image-only search/index orchestration has been removed for
client-ready, production-focused minimal code.
"""
# =============================================================================
# 1. Standard Library Imports
# =============================================================================
import logging
import copy
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
from datetime import datetime
import time
import tempfile
import json
import shutil
import re

import tqdm
import numpy as np
from PIL import Image

# =============================================================================
# 2. Application-Specific Imports
# =============================================================================
from .feature_extractor import FeatureExtractor
from .searcher import ImageSimilaritySearcher
from .faiss_manager import FaissIndexManager
try:
    from .qdrant_manager import QdrantManager  # optional
except Exception:
    QdrantManager = None  # type: ignore
    logging.getLogger(__name__).debug("Qdrant DB is not implemented or not available; will fallback to 'faiss' if provider='qdrant'.")
from .tif_utils import (
    parent_doc_from_db_stem,
    build_sim_img_checks_map,
    generate_tif_preview,
)
from .persistence import (
    write_tif_per_query_results_csv,
    try_save_tif_per_query_to_db,
)
from .constants import DEFAULT_REMOVE_COLUMNS_FROM_RESULTS, SIMILAR_DOC_FLAG_THRESHOLD
from .utils import detect_requesting_username
# Risk scoring is optional (Pro-only). Prefer premium module; fall back to
# neutral implementations when unavailable.
try:
    from .risk_scoring import compute_threshold_match_count, compute_fraud_probability  # type: ignore
except ImportError:
    from .risk_scoring_fallback import compute_threshold_match_count, compute_fraud_probability  # type: ignore
from .removal_filter import (
    effective_removal_set,
    filter_per_query_list,
    decide_global_top_docs_arg,
    decide_sim_img_check_arg,
)
from .authenticity_adapter import try_init_classifier
from .embedding_store import create_in_memory_store, create_disk_store, EmbeddingStore
from .augmentation_orchestrator import AugmentationOrchestrator
from .vector_db_base import VectorDBManager

# --- Import TIF processing modules (robust to both vendored and installed forms) ---
TifTextSearcher = None
PhotoExtractor = None
try:
    # Prefer the vendored version first
    from external.tif_searcher import TifTextSearcher
    from external.photo_extractor import PhotoExtractor
except ImportError:
    try:
        # Fallback to installed packages
        from tif_searcher import TifTextSearcher
        from photo_extractor import PhotoExtractor
    except ImportError:
        pass

# =============================================================================
# 3. Module-level Logger Setup
# =============================================================================
logger = logging.getLogger(__name__)

#

# =============================================================================
# 4. Helper Functions
# =============================================================================

def _resolve_path(path_str: Optional[str], project_root: Optional[Path] = None) -> Optional[Path]:
    """
    Resolves a string path to an absolute pathlib.Path object.

    If the path is relative, it's resolved against the provided project_root.

    Args:
        path_str (Optional[str]): The path string to resolve.
        project_root (Optional[Path]): Base path for resolving relative paths.

    Returns:
        Optional[Path]: A resolved, absolute Path object, or None if the input is None.
    """
    if not path_str:
        return None
    path_obj = Path(path_str)
    if path_obj.is_absolute():
        return path_obj.resolve()
    base_root = project_root if project_root else Path.cwd()
    return (base_root / path_obj).resolve()


def _compute_persist_query_crops_flag(search_task: Dict[str, Any]) -> bool:
    """Compute whether query image crops may be persisted, using privacy_mode only.

    Effective policy:
    - privacy_mode = True  -> persistence disallowed (return False)
    - privacy_mode = False -> persistence allowed   (return True)

    Args:
        search_task (Dict[str, Any]): The search_task configuration mapping.

    Returns:
        bool: True when persistence is allowed (privacy_mode is False), False otherwise.
    """
    try:
        if isinstance(search_task, dict):
            return not bool(search_task.get('privacy_mode', True))
    except Exception:
        # Defensive: default to privacy-first (no persistence)
        return False
    return False


def _initialize_provider_manager(
    provider: str,
    db_conf: Dict[str, Any],
    feature_extractor: FeatureExtractor,
    project_root: Path,
    search_task_conf: Dict[str, Any],
    full_config: Dict[str, Any],
    message_style: str = 'augmented',
) -> Tuple[Optional[VectorDBManager], int, Optional[str], Optional[str], Optional[str], Optional[str]]:
    """Initialize the vector search provider and decide fallback/auto-build.

    Centralizes provider setup for FAISS, Qdrant, and bruteforce, including:
    - Loading the base index/collection
    - Optional auto-build when load fails or an index is empty
    - Exact fallback policy and messages
    - Identifier string composition for run summaries

    Why: This removes duplicated provider initialization logic scattered across
    multiple workflows while preserving the exact behavior, outputs, and logs.

    Args:
        provider (str): Normalized provider name ('faiss' | 'qdrant' | 'bruteforce').
        db_conf (Dict[str, Any]): Vector database configuration mapping. Must include
            provider-specific sub-sections under keys 'faiss' and/or 'qdrant' when relevant,
            and global keys: 'allow_fallback', 'fallback_choice', 'build_index_on_load_failure'.
        feature_extractor (FeatureExtractor): Initialized feature extractor (for dim/model info).
        project_root (Path): Project root used to resolve relative paths.
        search_task_conf (Dict[str, Any]): The search_task configuration mapping (used for
            augmentation semantics and bruteforce DB folder behavior).
        full_config (Dict[str, Any]): Full configuration object. Only used to mirror existing
            auto-build behavior (preferring TIF-driven build when configured).
        message_style (str): Controls exact fallback message phrasing and, where applicable,
            legacy fallback behavior. Values: 'augmented' (default) retains current augmented
            path strings and semantics; 'legacy' reproduces the original non-augmented workflow
            strings (e.g., "FAISS index is not ready. ...") and Qdrant fallback_choice handling
            when the collection is empty.

    Returns:
        Tuple[Optional[VectorDBManager], int, Optional[str], Optional[str], Optional[str], Optional[str]]:
            - base_manager: Provider manager instance when loaded/retained; None when falling back to transient
              or bruteforce (or unsupported).
            - indexed_item_count: Total items in the loaded base; 0 when not ready or not applicable.
            - final_index_identifier: Provider identifier string for logs/summaries; None when not applicable.
            - fallback_to_bruteforce_message: Exact message string used when a fallback mode is selected; None otherwise.
            - selected_fallback_mode: 'transient' | 'bruteforce' | None. None is also used for Qdrant transient-only
              behavior to match prior semantics (merged transient index without explicit fallback flag).
            - fatal_error_message: Exact error message when fallback is disabled and base cannot be used; None otherwise.

    Raises:
        None: This function does not raise by design to preserve outer try/except framing.
    """
    # Normalize inputs
    provider = (provider or '').lower().strip()
    allow_fallback: bool = bool(db_conf.get('allow_fallback', True))
    fallback_choice: str = str(db_conf.get('fallback_choice', 'transient')).lower()
    if fallback_choice not in {"transient", "bruteforce"}:
        logger.warning("Unknown fallback_choice '%s'. Falling back to 'transient'.", fallback_choice)
        fallback_choice = "transient"
    build_index_on_load_failure: bool = bool(db_conf.get('build_index_on_load_failure', False))
    legacy_msgs: bool = (str(message_style).lower() == 'legacy')

    base_manager: Optional[VectorDBManager] = None
    indexed_item_count: int = 0
    final_index_identifier: Optional[str] = None
    fallback_to_bruteforce_message: Optional[str] = None
    selected_fallback_mode: Optional[str] = None  # 'transient' | 'bruteforce' | None
    fatal_error_message: Optional[str] = None

    # Helper: resolve path for auto-build preference
    def _resolve_optional_path(key: str) -> Optional[Path]:
        try:
            idx_cfg = full_config.get('indexing_task', {}) or {}
            p = idx_cfg.get(key)
            return _resolve_path(p, project_root) if p else None
        except Exception:
            return None

    # Localized message selectors to preserve exact strings across flows.
    def _faiss_not_ready_msg_transient() -> str:
        return (
            "FAISS index is not ready. Transient-only search will be used for this run."
            if legacy_msgs else "FAISS index not ready; using transient-only search for this run."
        )

    def _faiss_not_ready_msg_bruteforce() -> str:
        return (
            "FAISS index is not ready. Brute-force fallback will be used for this run."
            if legacy_msgs else "FAISS index not ready; will use brute-force fallback for this run."
        )

    def _faiss_init_error_msg_transient() -> str:
        return (
            "Error initializing FAISS manager. Transient-only search will be used for this run."
            if legacy_msgs else "Error initializing FAISS manager; using transient-only search for this run."
        )

    def _faiss_init_error_msg_bruteforce() -> str:
        return (
            "Error initializing FAISS manager. Brute-force fallback will be used for this run."
            if legacy_msgs else "Error initializing FAISS manager; using brute-force fallback for this run."
        )

    def _qdrant_empty_msg_transient() -> str:
        return (
            "Qdrant collection is empty or not ready. Transient-only search will be used for this run."
            if legacy_msgs else "Qdrant collection not ready; using transient-only search for this run."
        )

    def _qdrant_empty_msg_bruteforce() -> str:
        return (
            "Qdrant collection is empty or not ready. Brute-force fallback will be used for this run."
            if legacy_msgs else "Qdrant collection not ready; will use brute-force fallback for this run."
        )

    def _qdrant_init_error_msg_transient() -> str:
        return "Error or failure loading Qdrant; using transient-only search for this run."

    # FAISS provider
    if provider == 'faiss':
        faiss_conf = db_conf.get('faiss', {}) or {}
        try:
            vdb = FaissIndexManager(
                feature_dim=feature_extractor.feature_dim,
                output_directory=faiss_conf.get('output_directory'),
                filename_stem=faiss_conf.get('filename_stem', 'faiss_index'),
                index_type=faiss_conf.get('index_type', 'flat'),
                model_name=feature_extractor.model_name,
                faiss_config=faiss_conf,
                project_root_path=project_root,
            )

            def _try_load_faiss() -> bool:
                try:
                    return bool(vdb.load_index())
                except Exception:
                    return False

            loaded = _try_load_faiss()
            if not loaded and build_index_on_load_failure:
                logger.warning("FAISS index load failed; attempting automatic index build as configured (build_index_on_load_failure=true).")
                try:
                    tif_src = _resolve_optional_path('input_tif_folder_for_indexing')
                    if tif_src and tif_src.exists():
                        build_res = build_index_from_tif_folder_workflow(full_config, project_root)
                    else:
                        build_res = _build_image_index_from_folder(full_config, project_root)
                    logger.info(
                        "Auto index build result: status=%s, exit_code=%s, message=%s",
                        build_res.get('status'), build_res.get('exit_code'), build_res.get('message')
                    )
                except Exception as e_build_auto:
                    logger.error("Automatic index building failed: %s", e_build_auto, exc_info=True)
                # Recreate and retry
                vdb = FaissIndexManager(
                    feature_dim=feature_extractor.feature_dim,
                    output_directory=faiss_conf.get('output_directory'),
                    filename_stem=faiss_conf.get('filename_stem', 'faiss_index'),
                    index_type=faiss_conf.get('index_type', 'flat'),
                    model_name=feature_extractor.model_name,
                    faiss_config=faiss_conf,
                    project_root_path=project_root,
                )
                loaded = _try_load_faiss()

            if not loaded:
                if allow_fallback:
                    if fallback_choice == 'bruteforce':
                        fallback_to_bruteforce_message = "Could not load the 'faiss' index. Brute-force fallback will be used for this run."
                        selected_fallback_mode = 'bruteforce'
                    else:
                        fallback_to_bruteforce_message = "Could not load the 'faiss' index. Transient-only search will be used for this run."
                        selected_fallback_mode = 'transient'
                    logger.warning(fallback_to_bruteforce_message)
                    base_manager = None
                else:
                    fatal_error_message = "FAISS index could not be loaded and fallback is disabled."
            else:
                if vdb.is_index_loaded_and_ready():
                    base_manager = vdb
                    indexed_item_count = vdb.get_total_indexed_items()
                    final_index_identifier = (
                        f"FAISS sharded index: {len(vdb.index_identifiers)} shards (first: {vdb.index_identifiers[0]})"
                        if hasattr(vdb, 'index_identifiers') and len(getattr(vdb, 'index_identifiers', [])) > 1
                        else str(vdb.index_path.resolve())
                    )
                    logger.info("Base FAISS index loaded with %d items.", indexed_item_count)
                else:
                    # Loaded but empty
                    requested_aug_mode = str(search_task_conf.get('augmentation_mode', 'persistent_query_index')).lower()
                    if build_index_on_load_failure:
                        logger.warning("FAISS index not ready; attempting automatic index build as configured (build_index_on_load_failure=true).")
                        try:
                            tif_src = _resolve_optional_path('input_tif_folder_for_indexing')
                            if tif_src and tif_src.exists():
                                build_res = build_index_from_tif_folder_workflow(full_config, project_root)
                            else:
                                build_res = _build_image_index_from_folder(full_config, project_root)
                            logger.info(
                                "Auto index build result: status=%s, exit_code=%s, message=%s",
                                build_res.get('status'), build_res.get('exit_code'), build_res.get('message')
                            )
                        except Exception as e_build_empty:
                            logger.error("Automatic index building (empty) failed: %s", e_build_empty, exc_info=True)
                        # Recreate and retry
                        vdb = FaissIndexManager(
                            feature_dim=feature_extractor.feature_dim,
                            output_directory=faiss_conf.get('output_directory'),
                            filename_stem=faiss_conf.get('filename_stem', 'faiss_index'),
                            index_type=faiss_conf.get('index_type', 'flat'),
                            model_name=feature_extractor.model_name,
                            faiss_config=faiss_conf,
                            project_root_path=project_root,
                        )
                        try:
                            if vdb.load_index() and vdb.is_index_loaded_and_ready():
                                base_manager = vdb
                                indexed_item_count = vdb.get_total_indexed_items()
                                final_index_identifier = (
                                    f"FAISS sharded index: {len(vdb.index_identifiers)} shards (first: {vdb.index_identifiers[0]})"
                                    if hasattr(vdb, 'index_identifiers') and len(getattr(vdb, 'index_identifiers', [])) > 1
                                    else str(vdb.index_path.resolve())
                                )
                                logger.info("Base FAISS index loaded with %d items after auto-build.", indexed_item_count)
                        except Exception:
                            pass

                    if base_manager is None:
                        if requested_aug_mode == 'persistent_query_index':
                            logger.info("FAISS index not ready; proceeding with persistent augmentation (empty index will be initialized).")
                            base_manager = vdb  # keep manager to allow persistent augmentation
                            selected_fallback_mode = None
                        else:
                            if allow_fallback:
                                if fallback_choice == 'bruteforce':
                                    fallback_to_bruteforce_message = _faiss_not_ready_msg_bruteforce()
                                    selected_fallback_mode = 'bruteforce'
                                else:
                                    fallback_to_bruteforce_message = _faiss_not_ready_msg_transient()
                                    selected_fallback_mode = 'transient'
                                logger.warning(fallback_to_bruteforce_message)
                                base_manager = None
                            else:
                                fatal_error_message = "FAISS index not ready and fallback disabled."
        except Exception as e_f:
            logger.error("Error initializing FAISS manager: %s", e_f, exc_info=True)
            if allow_fallback:
                if fallback_choice == 'bruteforce':
                    fallback_to_bruteforce_message = _faiss_init_error_msg_bruteforce()
                    selected_fallback_mode = 'bruteforce'
                else:
                    fallback_to_bruteforce_message = _faiss_init_error_msg_transient()
                    selected_fallback_mode = 'transient'
                base_manager = None
            else:
                fatal_error_message = f"Initialization failed: {e_f}"
        return base_manager, indexed_item_count, final_index_identifier, fallback_to_bruteforce_message, selected_fallback_mode, fatal_error_message

    # Qdrant provider
    if provider == 'qdrant':
        qdrant_conf = db_conf.get('qdrant', {}) or {}
        try:
            if QdrantManager is None:
                # Strict optionality: when missing, apply transient-only fallback when allowed; else fatal error.
                if allow_fallback:
                    fallback_to_bruteforce_message = (
                        "Error or failure loading Qdrant; using transient-only search for this run."
                    )
                    logger.warning(fallback_to_bruteforce_message)
                    return None, 0, None, fallback_to_bruteforce_message, None, None
                return None, 0, None, None, None, "Qdrant index load failed and fallback disabled."

            vdb = QdrantManager(
                feature_dim=feature_extractor.feature_dim,
                collection_name_stem=qdrant_conf.get('collection_name_stem', 'image_similarity_collection'),
                model_name=feature_extractor.model_name,
                qdrant_config=qdrant_conf,
                project_root_path=project_root,
            )
            loaded_ok = False
            try:
                loaded_ok = bool(vdb.load_index())
            except Exception:
                loaded_ok = False

            def _compose_mode_desc() -> str:
                if qdrant_conf.get('location'):
                    loc = qdrant_conf.get('location')
                    return f"embedded:{loc}"
                host = qdrant_conf.get('host', 'localhost')
                port = qdrant_conf.get('port', 6333)
                return f"server:{host}:{port}"

            if loaded_ok and vdb.is_index_loaded_and_ready():
                base_manager = vdb
                indexed_item_count = vdb.get_total_indexed_items()
                final_index_identifier = f"qdrant:{vdb.collection_name} ({_compose_mode_desc()})"
                return base_manager, indexed_item_count, final_index_identifier, None, None, None

            if loaded_ok:
                # Loaded but empty
                if build_index_on_load_failure:
                    logger.warning("Qdrant collection not ready; attempting automatic index build as configured (build_index_on_load_failure=true).")
                    try:
                        cfg_auto = copy.deepcopy(full_config)
                        cfg_auto.setdefault('vector_database', {}).setdefault('qdrant', {})['force_recreate_collection'] = True
                        build_res = _build_image_index_from_folder(cfg_auto, project_root)
                        logger.info(
                            "Qdrant auto index build result: status=%s, exit_code=%s, message=%s",
                            build_res.get('status'), build_res.get('exit_code'), build_res.get('message')
                        )
                    except Exception as e_qauto:
                        logger.error("Automatic Qdrant index building failed: %s", e_qauto, exc_info=True)
                    # Recreate and retry
                    vdb = QdrantManager(
                        feature_dim=feature_extractor.feature_dim,
                        collection_name_stem=qdrant_conf.get('collection_name_stem', 'image_similarity_collection'),
                        model_name=feature_extractor.model_name,
                        qdrant_config=qdrant_conf,
                        project_root_path=project_root,
                    )
                    try:
                        if vdb.load_index() and vdb.is_index_loaded_and_ready():
                            base_manager = vdb
                            indexed_item_count = vdb.get_total_indexed_items()
                            final_index_identifier = f"qdrant:{vdb.collection_name} ({_compose_mode_desc()})"
                            return base_manager, indexed_item_count, final_index_identifier, None, None, None
                    except Exception:
                        pass

                requested_aug_mode = str(search_task_conf.get('augmentation_mode', 'persistent_query_index')).lower()
                if requested_aug_mode == 'persistent_query_index':
                    logger.info("Qdrant collection not ready; proceeding with persistent augmentation (empty collection will be initialized).")
                    base_manager = vdb
                    indexed_item_count = 0
                    final_index_identifier = f"qdrant:{vdb.collection_name} ({_compose_mode_desc()})"
                    return base_manager, indexed_item_count, final_index_identifier, None, None, None

                if allow_fallback:
                    if legacy_msgs and fallback_choice == 'bruteforce':
                        fallback_to_bruteforce_message = _qdrant_empty_msg_bruteforce()
                        logger.warning(fallback_to_bruteforce_message)
                        return None, 0, None, fallback_to_bruteforce_message, 'bruteforce', None
                    else:
                        fallback_to_bruteforce_message = _qdrant_empty_msg_transient()
                        logger.warning(fallback_to_bruteforce_message)
                        return None, 0, None, fallback_to_bruteforce_message, None, None

                return None, 0, None, None, None, "Qdrant index not ready and fallback disabled."

            # Failed to load; optionally auto-build first
            if build_index_on_load_failure:
                logger.warning("Error or failure loading Qdrant; attempting automatic index build as configured (build_index_on_load_failure=true).")
                try:
                    cfg_auto = copy.deepcopy(full_config)
                    cfg_auto.setdefault('vector_database', {}).setdefault('qdrant', {})['force_recreate_collection'] = True
                    build_res = _build_image_index_from_folder(cfg_auto, project_root)
                    logger.info(
                        "Qdrant auto index build result: status=%s, exit_code=%s, message=%s",
                        build_res.get('status'), build_res.get('exit_code'), build_res.get('message')
                    )
                except Exception as e_qauto2:
                    logger.error("Automatic Qdrant index building failed: %s", e_qauto2, exc_info=True)
                # Recreate and retry
                vdb = QdrantManager(
                    feature_dim=feature_extractor.feature_dim,
                    collection_name_stem=qdrant_conf.get('collection_name_stem', 'image_similarity_collection'),
                    model_name=feature_extractor.model_name,
                    qdrant_config=qdrant_conf,
                    project_root_path=project_root,
                )
                try:
                    if vdb.load_index() and vdb.is_index_loaded_and_ready():
                        base_manager = vdb
                        indexed_item_count = vdb.get_total_indexed_items()
                        final_index_identifier = f"qdrant:{vdb.collection_name} ({_compose_mode_desc()})"
                        return base_manager, indexed_item_count, final_index_identifier, None, None, None
                    else:
                        if allow_fallback:
                            fallback_to_bruteforce_message = "Error or failure loading Qdrant; using transient-only search for this run."
                            logger.warning(fallback_to_bruteforce_message)
                            return None, 0, None, fallback_to_bruteforce_message, None, None
                        return None, 0, None, None, None, "Qdrant index load failed and fallback disabled."
                except Exception:
                    if allow_fallback:
                        fallback_to_bruteforce_message = "Error or failure loading Qdrant; using transient-only search for this run."
                        logger.warning(fallback_to_bruteforce_message)
                        return None, 0, None, fallback_to_bruteforce_message, None, None
                    return None, 0, None, None, None, "Qdrant index load failed and fallback disabled."

            if allow_fallback:
                fallback_to_bruteforce_message = "Error or failure loading Qdrant; using transient-only search for this run."
                logger.warning(fallback_to_bruteforce_message)
                return None, 0, None, fallback_to_bruteforce_message, None, None
            return None, 0, None, None, None, "Qdrant index load failed and fallback disabled."
        except Exception as e_q:
            logger.error("Error initializing Qdrant manager: %s", e_q, exc_info=True)
            if allow_fallback:
                if legacy_msgs and fallback_choice == 'bruteforce':
                    fallback_to_bruteforce_message = "Error initializing Qdrant manager. Brute-force fallback will be used for this run."
                    return None, 0, None, fallback_to_bruteforce_message, 'bruteforce', None
                else:
                    fallback_to_bruteforce_message = "Error initializing Qdrant manager. Transient-only search will be used for this run."
                    return None, 0, None, fallback_to_bruteforce_message, None, None
            return None, 0, None, None, None, f"Initialization failed: {e_q}"

    # Bruteforce provider
    if provider == 'bruteforce':
        return None, 0, None, None, 'bruteforce', None

    # Unsupported provider
    return None, 0, None, None, None, f"Unsupported provider '{provider}'."

# =============================================================================
# 5. TIF Workflows
# =============================================================================

if TifTextSearcher is None or PhotoExtractor is None:
    logger.error("TIF processing libraries not available. Either keep them under 'external' or install 'tif-searcher' and 'photo-extractor'.")


def _execute_tif_batch_search_with_augmentation(
    config: Dict[str, Any],
    project_root: Path,
    input_folder: Path,
    base_output_folder: Path,
    run_root: Path,
    run_identifier: str,
    requesting_username: str,
) -> Dict[str, Any]:
    """Augmented two-phase TIF batch search (privacy-first, batch-first).

    Overview
    - Phase A (extract once, embed once):
      Iterate TIFs in input_folder, find OCR-matched pages, extract photos, compute embeddings in-memory (no crop
      persistence), and persist vectors + minimal metadata into a disk-backed EmbeddingStore.
    - Augmentation:
      Depending on augmentation_mode, batch vectors augment the main provider (FAISS/Qdrant) and/or populate a transient
      per-run index.
    - Phase B (search using Phase A embeddings):
      For each query TIF, load its vectors and perform merged search (base + transient). Aggregate results to parent
      TIF docs and produce both per-query and global outputs.

    Output fields parity with the legacy path
    - image_authenticity: dict[str, str]
        When photo_authenticity_classifier_config.check_image_authenticity is true and classifier initializes, each
        query photo is inferred and mapped as "<tif_stem>_q_<idx>.jpg" -> "<class_name>". On any failure or when the
        classifier is disabled/unavailable, this degrades to {}.
    - fraud_doc_probability: str
        Predicted risk based on high-similarity evidence and authenticity classes using threshold rules:
          similar_doc_flag_threshold = YAML photo_authenticity_classifier_config.similar_doc_flag_threshold
                                        or constants.SIMILAR_DOC_FLAG_THRESHOLD
          Very_High if any top_docs score >= similar_doc_flag_threshold AND any photo is in
                      non_authentic_image_classes
          High      if either condition holds
          No        otherwise
    - threshold_match_count: int
        Count of items in the per-query top_docs list whose score >= constants.SIMILAR_DOC_FLAG_THRESHOLD. This value is
        exported in the per-query CSV (subject to removal).
    - global_top_docs: list[str]
        Names of globally ranked top documents. Written into the per-query CSV as a JSON array only when
        search_task.save_global_top_docs is true and 'global_top_docs' is not listed in removal settings. If removed,
        this column is omitted entirely, and the per-query CSV writer is passed None.

    Column removal (consistent across CSV and JSON)
    - Effective removal set is taken from search_task.remove_columns_from_results when provided; otherwise defaults to
      constants.DEFAULT_REMOVE_COLUMNS_FROM_RESULTS (empty list by default).
    - For the run summary JSON, keys listed in the removal set are deleted from each per-query entry. If
      'top_similar_docs' is removed, the internal 'top_docs' field (used for CSV shaping) is also removed.
    - For the per-query CSV, headers are filtered by the removal set. threshold_match_count participates in the same
      filtering logic as any other column.

    Privacy gating
    - When search_task.privacy_mode is true, any image copying, preview generation and sim_img_check payloads
      are suppressed. Authenticity inference remains in-memory and is allowed.

    Parameters
    - config: Full search configuration (YAML-derived dict)
    - project_root: Absolute project root
    - input_folder: Folder containing query .tif/.tiff files
    - base_output_folder: Root folder for outputs
    - run_root: Concrete run directory where outputs may be written
    - run_identifier: Stable identifier for this run
    - requesting_username: For audit columns and file naming

    Returns
    - Dict with keys: status, exit_code, message, top_documents, per_query, indexed_image_count, index_path,
      config_was_modified, fallback_to_bruteforce_message, tif_run_output_path (optional), tif_run_summary_json_path
      (optional), tif_run_db_export_csv_path (optional)
    """
    try:
        cloned_config = copy.deepcopy(config)
        st = cloned_config.get('search_task', {}) or {}

        # Privacy-mode guardrails + output suppression for legacy artifacts
        persist_query_crops_flag = _compute_persist_query_crops_flag(st)
        if not persist_query_crops_flag:
            # Explicitly disable any operations that would persist or preview images
            # Keep sim_img_check available for DB-only debugging when enabled.
            st['copy_query_image_to_output'] = False
            st['copy_similar_images_to_output'] = False
            st['generate_tif_previews'] = False

        # Resolve input folder
        if not input_folder or not input_folder.exists():
            return {"status": "error", "exit_code": 1, "message": "Input TIF folder for augmented search not found."}

        # Output persistence flags
        create_new_subfolder = bool(st.get('new_query_new_subfolder', True))
        save_outputs_to_folder = bool(st.get('save_outputs_to_folder', True))

        # Search parameters
        top_k = int(st.get('top_k', 5))
        top_doc = int(st.get('top_doc', 7))
        top_doc_global = int(st.get('top_doc_global', top_doc))
        aggregation_strategy = str(st.get('aggregation_strategy', 'max')).lower()
        if aggregation_strategy not in {"max", "sum", "mean"}:
            aggregation_strategy = "max"
        include_query_image_to_result = bool(st.get('include_query_image_to_result', False))

        # Optional photo authenticity classifier (augmented path)
        pac = cloned_config.get('photo_authenticity_classifier_config') or {}
        check_auth = bool(pac.get('check_image_authenticity', False))
        non_auth_classes = list(pac.get('non_authentic_image_classes', []) or [])
        try:
            similar_doc_flag_threshold = float(pac.get('similar_doc_flag_threshold', SIMILAR_DOC_FLAG_THRESHOLD))
        except Exception:
            similar_doc_flag_threshold = SIMILAR_DOC_FLAG_THRESHOLD
        classifier_cfg_path = pac.get('classifier_config_path')
        classifier = None
        if check_auth:
            cfg_path_resolved = _resolve_path(classifier_cfg_path, project_root) if classifier_cfg_path else None
            classifier = try_init_classifier(str(cfg_path_resolved) if cfg_path_resolved else None)

        # Initialize feature extractor
        feature_extractor = FeatureExtractor(cloned_config.get('feature_extractor', {}), project_root)

        # Initialize base provider (if configured)
        db_conf = cloned_config.get('vector_database', {}) or {}
        provider = str(db_conf.get('provider', 'faiss')).lower()
        faiss_conf = db_conf.get('faiss', {}) or {}
        qdrant_conf = db_conf.get('qdrant', {}) or {}
        allow_fallback = bool(db_conf.get('allow_fallback', True))
        if provider == 'qdrant' and QdrantManager is None:
            logger.warning("Qdrant DB is not implemented or not available; falling back to 'faiss'.")
            provider = 'faiss'

        base_manager: Optional[Any] = None
        final_index_file_path_str: Optional[str] = None
        indexed_item_count: int = 0
        fallback_to_bruteforce_message: Optional[str] = None
        # Fallback mode selection and auto-build flag
        fallback_choice: str = str(db_conf.get('fallback_choice', 'transient')).lower()
        if fallback_choice not in {"transient", "bruteforce"}:
            logger.warning("Unknown fallback_choice '%s'. Falling back to 'transient'.", fallback_choice)
            fallback_choice = "transient"
        build_index_on_load_failure: bool = bool(db_conf.get('build_index_on_load_failure', False))
        # Internal fallback mode tracker for Phase B
        selected_fallback_mode: Optional[str] = None  # 'transient' | 'bruteforce' | None

        # Centralized provider initialization
        base_manager, indexed_item_count, final_index_file_path_str, fallback_to_bruteforce_message, selected_fallback_mode, _fatal = _initialize_provider_manager(
            provider=provider,
            db_conf=db_conf,
            feature_extractor=feature_extractor,
            project_root=project_root,
            search_task_conf=st,
            full_config=cloned_config,
        )
        if _fatal:
            return {"status": "error", "exit_code": 1, "message": _fatal}

        # Orchestrator: governs augmentation and merged search
        augmentation_mode = str(st.get('augmentation_mode', 'persistent_query_index')).lower()
        transient_batch_index = bool(st.get('transient_batch_index', True))
        orch = AugmentationOrchestrator(
            provider=provider,
            base_manager=base_manager,
            faiss_search_config=faiss_conf,
            persist_query_crops=persist_query_crops_flag,
            augmentation_mode=augmentation_mode,
            transient_batch_index=transient_batch_index,
            feature_dim=feature_extractor.feature_dim,
            tmp_dir=(run_root if save_outputs_to_folder else None),
        )
        # Validate privacy vs provider
        try:
            orch.validate_privacy_vs_provider()
        except Exception as e_priv:
            return {"status": "error", "exit_code": 1, "message": str(e_priv)}

        # If FAISS failed and selected fallback is 'transient', keep base_manager=None (already).
        # If 'bruteforce', we will rely on brute-force DB folder later during per-photo searches.

        # EmbeddingStore setup (disk-backed by default for scalability)
        # Default base: instance/query_image_embeddings
        # Override via: search_task.query_embed_index.output_path_query_embed_index
        qei_cfg = cloned_config.get('search_task', {}).get('query_embed_index', {}) or {}
        base_dir_str = qei_cfg.get('output_path_query_embed_index', 'instance/query_image_embeddings')
        base_dir = Path(base_dir_str)
        if not base_dir.is_absolute():
            base_dir = (project_root / base_dir).resolve()
        emb_store_dir = base_dir / run_identifier
        emb_store_dir.mkdir(parents=True, exist_ok=True)
        store = create_disk_store(emb_store_dir, vector_dim=feature_extractor.feature_dim, shard_size=int(st.get('embedding_shard_size', 20000)), dtype=np.float16, run_id=run_identifier)

        # Initialize search helper for Phase B, honoring selected fallback mode
        searcher = ImageSimilaritySearcher(
            feature_extractor=feature_extractor,
            vector_db_manager=base_manager,
            fallback_mode=selected_fallback_mode,
        )

        # Initialize TIF components
        searcher_conf = cloned_config.get('searcher_config') or {}
        tif_searcher_kwargs = searcher_conf if isinstance(searcher_conf, dict) and len(searcher_conf) > 0 else None
        tif_searcher = TifTextSearcher(**tif_searcher_kwargs) if tif_searcher_kwargs else TifTextSearcher()

        pe_conf = cloned_config.get('photo_extractor_config', {}) or {}
        photo_extraction_mode = str(pe_conf.get('photo_extraction_mode', 'bbox')).lower()
        bbox_conf = pe_conf.get('bbox_extraction', {}) or {}
        bbox_list = bbox_conf.get('bbox_list', [])
        bbox_format = str(bbox_conf.get('bbox_format', 'xyxy'))
        bbox_normalized = bool(bbox_conf.get('normalized', False))
        if photo_extraction_mode == 'bbox':
            pe_conf_override = {'photo_extraction_mode': 'bbox', 'bbox_extraction': bbox_conf}
            photo_extractor = PhotoExtractor(config_override=pe_conf_override)
        else:
            photo_extractor = PhotoExtractor(config_override=pe_conf)
        pe_debug = bool(pe_conf.get('photo_extractor_debug', False))

        # Collect TIF files
        query_tif_files = list(input_folder.glob('*.tif')) + list(input_folder.glob('*.tiff'))
        if not query_tif_files:
            msg = f"No query .tif files found in {input_folder}. Exiting."
            logger.warning(msg)
            return {
                "status": "success", "exit_code": 0, "message": msg,
                "top_documents": [], "per_query": [],
                "indexed_image_count": indexed_item_count, "index_path": final_index_file_path_str,
                "config_was_modified": False, "fallback_to_bruteforce_message": fallback_to_bruteforce_message,
            }

        # ---------------------
        # Phase A: embeddings
        # ---------------------
        total_photos = 0
        for tif_path in tqdm.tqdm(query_tif_files, desc="Phase A: embedding query photos"):
            try:
                parent_name = tif_path.name
                page_numbers = tif_searcher.find_text_pages(tif_path)
                if not page_numbers:
                    continue
                photos_batch: List[np.ndarray] = []
                item_names: List[str] = []
                for page_num in page_numbers:
                    if photo_extraction_mode == 'yolo':
                        photos = photo_extractor.extract_photos(tif_path, page_num)
                    else:
                        with Image.open(tif_path) as _tif_img:
                            if not (1 <= page_num <= _tif_img.n_frames):
                                continue
                    # Note: per-phase fallback mode 'selected_fallback_mode' is handled by leaving base_manager=None
                    # (transient) or using brute-force DB folder resolved later when needed.
                            _tif_img.seek(page_num - 1)
                            page_image = _tif_img.convert('RGB')
                            photos = photo_extractor.extract_photos_from_bboxes(
                                page_image, bboxes=bbox_list, bbox_format=bbox_format, normalized=bbox_normalized,
                            )
                            if not photos:
                                photos = photo_extractor.extract_photos_from_bboxes(
                                    page_image, bboxes=[[0.0, 0.0, 1.0, 1.0]], bbox_format='xyxy', normalized=True,
                                )
                    for idx, pil_img in enumerate(photos or []):
                        try:
                            np_img = np.asarray(pil_img.convert('RGB'))
                        except Exception:
                            continue
                        photos_batch.append(np_img)
                        item_names.append(f"{tif_path.stem}_page{page_num}_photo{idx}.jpg")
                if photos_batch:
                    vecs = feature_extractor.extract_features(photos_batch)
                    # Ensure 2D
                    if vecs.ndim == 1:
                        vecs = vecs.reshape(1, -1)
                    store.add_batch(parent_names=[parent_name] * vecs.shape[0], item_names=item_names, vectors=vecs)
                    total_photos += vecs.shape[0]
            except Exception as e_emb:
                logger.warning("Embedding failed for '%s': %s", tif_path.name, e_emb)
        store.finalize()
        total_embeddings = store.total_count()
        logger.info(
            "Phase A: extracted %d photos across %d TIF(s); computed %d embeddings; no crops persisted.",
            total_photos, len(query_tif_files), total_embeddings
        )

        # -------------------------
        # Augmentation (persistent|transient)
        # -------------------------
        # Load all vectors for augmentation (can be optimized to stream per shard if needed)
        all_vecs_list: List[np.ndarray] = []
        all_names: List[str] = []

        # Resolve brute-force DB folder for fallback searches (if configured)
        bruteforce_db_folder = st.get('bruteforce_db_folder') or cloned_config.get('indexing_task', {}).get('image_folder_to_index')
        bruteforce_db_folder_path = _resolve_path(bruteforce_db_folder, project_root) if bruteforce_db_folder else None
        for parent in store.iter_parents():
            vecs, names = store.load_vectors_for_parent(parent)
            if vecs.size == 0:
                continue
            all_vecs_list.append(vecs)
            all_names.extend(names)
        if all_vecs_list:
            all_vecs = np.vstack(all_vecs_list)
            try:
                orch.augment_with_batch(all_vecs.astype(np.float32, copy=False), all_names)
            except Exception as e_aug:
                # When persistent augmentation fails (e.g., IVF governance), try transient if policy allows
                logger.error("Augmentation failed: %s", e_aug)
                return {"status": "error", "exit_code": 1, "message": str(e_aug)}
        else:
            logger.info("No embeddings produced in Phase A; skipping augmentation and Phase B.")
            return {
                "status": "success", "exit_code": 0, "message": "No embeddings produced from input; nothing to search.",
                "top_documents": [], "per_query": [],
                "indexed_image_count": indexed_item_count, "index_path": final_index_file_path_str,
                "config_was_modified": False, "fallback_to_bruteforce_message": fallback_to_bruteforce_message,
            }

        # ---------------------
        # Phase B: similarity search
        # ---------------------
        document_scores: Dict[str, float] = {}
        per_query_results: List[Dict[str, Any]] = []
        per_doc_neighbors: Dict[str, List[Dict[str, Any]]] = {}
        per_query_timestamps: Dict[str, str] = {}
        name_map: Dict[str, str] = (cloned_config.get('key_input_runtime', {}) or {}).get('query_name_map', {}) or {}

        # Output/copy flags (already privacy-gated above when needed)
        generate_previews = bool(st.get('generate_tif_previews', True))
        create_per_query_subfolders = bool(st.get('create_per_query_subfolders_for_tif', True))
        save_search_summary_json = bool(st.get('save_search_summary_json', True))

        
        for tif_path in tqdm.tqdm(query_tif_files, desc="Phase B: searching (TIFs)"):
            t0 = time.perf_counter()
            matched_name = tif_path.name
            original_name = name_map.get(matched_name, matched_name)

            # Reuse vectors for this query from store
            q_vecs, q_names = store.load_vectors_for_parent(matched_name)
            if q_vecs.size == 0:
                per_query_results.append({
                    "query_document": original_name,
                    "matched_query_document": matched_name,
                    "num_query_photos": 0,
                    "top_docs": [],
                    "elapsed_seconds": 0.0,
                    "top_k_used": top_k,
                    "top_doc_used": top_doc,
                    "aggregation_strategy_used": aggregation_strategy,
                    "threshold_match_count": 0,
                })
                continue

            # For each vector, query merged search (base + transient)
            per_doc_scores_local: Dict[str, List[float]] = {}
            local_per_doc_neighbors: Dict[str, List[Dict[str, Any]]] = {}
            for i in range(q_vecs.shape[0]):
                qv = q_vecs[i]
                # If selected fallback is 'transient', merged_search (base=None + transient) is fine.
                # If selected fallback is 'bruteforce', run brute-force directly using query vectors.
                if selected_fallback_mode == 'bruteforce' and bruteforce_db_folder_path:
                    merged = searcher.search_similar_by_vector_bruteforce(
                        query_vector=qv.astype(np.float32, copy=False),
                        top_k=top_k,
                        db_folder_for_bruteforce=bruteforce_db_folder_path,
                        bruteforce_batch_size=int(st.get('bruteforce_batch_size', 32)),
                    )
                else:
                    merged = orch.merged_search(
                        query_vector=qv.astype(np.float32, copy=False),
                        top_k=top_k,
                        include_query_image_to_result=include_query_image_to_result,
                        query_parent_document_name=matched_name,
                    )
                for name_or_path, score in merged:
                    try:
                        stem = Path(name_or_path).stem
                    except Exception:
                        stem = Path(str(name_or_path)).stem
                    parent_doc = parent_doc_from_db_stem(stem)
                    per_doc_scores_local.setdefault(parent_doc, []).append(float(score))
                    entry = {"path": str(name_or_path), "score": float(score), "query_photo_id": i}
                    per_doc_neighbors.setdefault(parent_doc, []).append(entry)
                    local_per_doc_neighbors.setdefault(parent_doc, []).append(entry)
                    # Global max tracker
                    prev = document_scores.get(parent_doc)
                    if prev is None or score > prev:
                        document_scores[parent_doc] = float(score)
                    # Note: per-phase fallback mode 'selected_fallback_mode' is handled by leaving base_manager=None
                    # (transient) or using brute-force DB folder resolved later when needed.

            # Aggregate per-query
            agg_scores: Dict[str, float] = {}
            for doc_name, scores in per_doc_scores_local.items():
                if not scores:
                    continue
                if aggregation_strategy == 'max':
                    agg_val = max(scores)
                elif aggregation_strategy == 'sum':
                    agg_val = float(np.sum(np.array(scores, dtype=float)))
                else:
                    agg_val = float(np.mean(np.array(scores, dtype=float)))
                agg_scores[doc_name] = float(agg_val)

            top_docs_for_query = [
                {"document": d, "score": s}
                for d, s in sorted(agg_scores.items(), key=lambda kv: kv[1], reverse=True)[:top_doc]
            ]

            # Compute threshold-based count for CSV (using centralized utility)
            threshold_match_count = compute_threshold_match_count(top_docs_for_query, SIMILAR_DOC_FLAG_THRESHOLD)

            # Authenticity inference (in-memory) and fraud probability
            image_auth_map_local: Dict[str, str] = {}
            has_non_authentic_photo: bool = False
            if check_auth and classifier is not None:
                try:
                    page_numbers_local = tif_searcher.find_text_pages(tif_path)
                    for page_num in page_numbers_local or []:
                        if photo_extraction_mode == 'yolo':
                            photos = photo_extractor.extract_photos(tif_path, page_num)
                        else:
                            with Image.open(tif_path) as _tif_img:
                                if not (1 <= page_num <= _tif_img.n_frames):
                                    continue
                                _tif_img.seek(page_num - 1)
                                page_image = _tif_img.convert('RGB')
                                photos = photo_extractor.extract_photos_from_bboxes(
                                    page_image, bboxes=bbox_list, bbox_format=bbox_format, normalized=bbox_normalized,
                                )
                                if not photos:
                                    photos = photo_extractor.extract_photos_from_bboxes(
                                        page_image, bboxes=[[0.0, 0.0, 1.0, 1.0]], bbox_format='xyxy', normalized=True,
                                    )
                        for idx, pil_img in enumerate(photos or []):
                            try:
                                result = classifier.infer(pil_img)
                                detected_class = str((result or {}).get('class_name') or 'unknown')
                            except Exception:
                                detected_class = 'unknown'
                            image_auth_map_local[f"{tif_path.stem}_q_{idx}.jpg"] = detected_class
                            if detected_class in non_auth_classes:
                                has_non_authentic_photo = True
                except Exception as e_auth:
                    logger.warning("Auth classifier inference failed (augmented) for '%s': %s. Skipping authenticity checks for this query.", tif_path.name, e_auth)
                    image_auth_map_local = {}
                    has_non_authentic_photo = False

            fraud_doc_probability = compute_fraud_probability(
                top_docs_for_query,
                non_auth_classes,
                image_auth_map_local,
                similar_doc_flag_threshold,
            )

            elapsed = time.perf_counter() - t0
            per_query_timestamps[original_name] = datetime.now().isoformat()

            per_query_entry: Dict[str, Any] = {
                "query_document": original_name,
                "matched_query_document": matched_name,
                "num_query_photos": int(q_vecs.shape[0]),
                "top_docs": top_docs_for_query,
                "elapsed_seconds": elapsed,
                "top_k_used": top_k,
                "top_doc_used": top_doc,
                "aggregation_strategy_used": aggregation_strategy,
                "image_authenticity": image_auth_map_local,
                "fraud_doc_probability": fraud_doc_probability,
                "threshold_match_count": int(threshold_match_count),
            }
            per_query_results.append(per_query_entry)

            # Copy query and ranked parent docs when enabled (privacy already gated)
            per_query_folder = (run_root / tif_path.name) if create_per_query_subfolders else None
            if save_outputs_to_folder and st.get('copy_query_image_to_output', True):
                try:
                    base_dir_for_query_copy = per_query_folder if create_per_query_subfolders else run_root
                    base_dir_for_query_copy.mkdir(parents=True, exist_ok=True)
                    q_dst_dir = base_dir_for_query_copy / "_query_image_source" if st.get('save_query_in_separate_subfolder_if_copied', True) else base_dir_for_query_copy
                    q_dst_dir.mkdir(exist_ok=True)
                    shutil.copy2(tif_path, q_dst_dir / tif_path.name)
                    if generate_previews:
                        try:
                            preview_path = q_dst_dir / f"{tif_path.stem}_preview.jpg"
                            generate_tif_preview(tif_path, preview_path)
                        except Exception as e_prev:
                            logger.warning("Failed to create preview for query TIF '%s': %s", tif_path, e_prev)
                except Exception as e_copy:
                    logger.warning("Failed to copy query TIF '%s': %s", tif_path, e_copy)

            if save_outputs_to_folder and st.get('copy_similar_images_to_output', True) and top_docs_for_query:
                if not create_per_query_subfolders:
                    logger.info("Per-query subfolders disabled; skipping per-query ranked copies in augmented mode.")
                else:
                    per_query_folder.mkdir(parents=True, exist_ok=True)
                    for rank, item in enumerate(top_docs_for_query, 1):
                        parent_name = item['document']
                        src = None
                        for candidate_root in [st.get('input_tif_folder_for_search'), cloned_config.get('indexing_task', {}).get('input_tif_folder_for_indexing')]:
                            if not candidate_root:
                                continue
                            root_path = _resolve_path(candidate_root, project_root)
                            candidate_path = root_path / parent_name
                            if candidate_path.is_file():
                                src = candidate_path
                                break
                            if root_path and root_path.exists():
                                try:
                                    src = next(root_path.rglob(parent_name))
                                    break
                                except StopIteration:
                                    pass
                        if src is None:
                            logger.warning("Parent TIF '%s' not found in configured folders. Skipping copy.", parent_name)
                            continue
                        dest = per_query_folder / f"ranked_{rank:02d}_{Path(parent_name).name}"
                        try:
                            shutil.copy2(src, dest)
                            if generate_previews:
                                try:
                                    _base = dest.with_suffix('')
                                    preview_path = _base.with_name(_base.name + "_preview.jpg")
                                    generate_tif_preview(dest, preview_path)
                                except Exception as e_prev:
                                    logger.warning("Failed to create preview for ranked TIF '%s': %s", dest, e_prev)
                        except Exception as e_copy:
                            logger.warning("Failed to copy parent TIF '%s' -> '%s': %s", src, dest, e_copy)

            # Save per-query summary JSON inside its folder when enabled
            if save_outputs_to_folder and save_search_summary_json and per_query_folder:
                try:
                    per_query_folder.mkdir(parents=True, exist_ok=True)
                    per_query_summary = {
                        "global": {
                            "aggregation_strategy_used": aggregation_strategy,
                            "top_k_used": top_k,
                            "top_doc_used": top_doc,
                            "fallback_to_bruteforce_message": fallback_to_bruteforce_message,
                            "indexed_image_count": indexed_item_count,
                            "index_path": final_index_file_path_str,
                        },
                        "per_query": [per_query_entry],
                    }
                    with open(per_query_folder / 'search_results_summary.json', 'w', encoding='utf-8') as f:
                        json.dump(per_query_summary, f, ensure_ascii=False, indent=2)
                except Exception as e_jsq:
                    logger.warning("Failed to write per-query summary for '%s': %s", tif_path.name, e_jsq)

        # ---------------------
        # Global aggregation and outputs
        # ---------------------
        if not document_scores:
            msg = "No similar documents found across all queries."
            return {
                "status": "success", "exit_code": 0, "message": msg,
                "top_documents": [], "per_query": per_query_results,
                "indexed_image_count": indexed_item_count, "index_path": final_index_file_path_str,
                "config_was_modified": False, "fallback_to_bruteforce_message": fallback_to_bruteforce_message,
                "tif_run_output_path": str(run_root.resolve()) if save_outputs_to_folder else None,
            }

        final_top_documents = [
            {"document": doc, "score": score}
            for doc, score in sorted(document_scores.items(), key=lambda item: item[1], reverse=True)[:top_doc_global]
        ]

        # Optional removal of columns in outputs
        removal_list = list((st or {}).get('remove_columns_from_results', DEFAULT_REMOVE_COLUMNS_FROM_RESULTS) or [])
        removal_set = effective_removal_set(removal_list, DEFAULT_REMOVE_COLUMNS_FROM_RESULTS)
        # Global-top names for CSV/JSON depending on config/removal
        save_global_top_docs = bool(st.get('save_global_top_docs', False))
        global_top_docs_names: List[str] = [item['document'] for item in final_top_documents] if final_top_documents else []
        def _filter_per_query_for_json(items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
            """Filter per-query entries for JSON output according to removal_set.

            Args:
                items (List[Dict[str, Any]]): Raw per-query entries.

            Returns:
                List[Dict[str, Any]]: Filtered entries with keys removed per removal_set;
                also drops internal 'top_docs' when 'top_similar_docs' is removed.
            """
            filtered: List[Dict[str, Any]] = []
            for entry in items:
                e = dict(entry)
                # Remove any keys explicitly requested
                for k in list(e.keys()):
                    if k in removal_set:
                        e.pop(k, None)
                # If top_similar_docs is removed, also drop the internal top_docs
                if 'top_similar_docs' in removal_set:
                    e.pop('top_docs', None)
                filtered.append(e)
            return filtered

        run_summary_path: Optional[Path] = None
        if save_outputs_to_folder and st.get('save_search_summary_json', True):
            global_section = {
                "aggregation_strategy_used": aggregation_strategy,
                "top_k_used": top_k,
                "top_doc_used": top_doc,
                "fallback_to_bruteforce_message": fallback_to_bruteforce_message,
                "indexed_image_count": indexed_item_count,
                "index_path": final_index_file_path_str,
            }
            if save_global_top_docs and ('global_top_docs' not in removal_set):
                try:
                    global_section["global_top_docs"] = global_top_docs_names
                except Exception:
                    global_section["global_top_docs"] = []
            run_summary = {"global": global_section, "per_query": _filter_per_query_for_json(per_query_results)}
            summary_filename = Path(st.get('search_summary_json_filename', 'search_results_summary.json')).name
            run_summary_path = run_root / summary_filename
            try:
                with open(run_summary_path, 'w', encoding='utf-8') as f:
                    json.dump(run_summary, f, ensure_ascii=False, indent=2)
                logger.info("Saved run summary JSON to '%s'", run_summary_path)
            except Exception as e_js:
                logger.error("Failed to write run summary JSON: %s", e_js)

        # Build optional sim_img_check for DB payloads (allowed in privacy mode; DB-only)
        sim_checks_map: Optional[Dict[str, Any]] = None
        if st.get('doc_sim_img_check', False):
            try:
                max_k_cap = st.get('doc_sim_img_check_max_k')
                sim_checks_map = build_sim_img_checks_map(final_top_documents, per_doc_neighbors, top_k, max_k_cap)
            except Exception as e_sim:
                logger.warning("Failed to build sim_img_check map: %s", e_sim)
                sim_checks_map = None

        # Doc-level PostgreSQL save removed: use per-query DB save only to mirror CSV.
        tif_run_db_export_csv_path: Optional[str] = None

        # Per-query DB save (augmented path): mirror CSV columns/logic
        try:
            if cloned_config.get('search_task', {}).get('save_results_to_postgresql', False):
                key_enrichment = cloned_config.get('key_enrichment') or {}
                ke_columns: List[str] = list(key_enrichment.get('columns_for_results') or [])
                per_query_extra_values: Dict[str, Any] = key_enrichment.get('per_query_metadata_map') or {}
                per_query_sim_checks_payload: Optional[Dict[str, Any]] = None
                if 'sim_img_check' not in removal_set:
                    tmp_map: Dict[str, Any] = {}
                    for pq in per_query_results:
                        qname = pq.get('query_document')
                        if not qname:
                            continue
                        doc_entries: Dict[str, Any] = {}
                        for td in (pq.get('top_docs') or []):
                            docname = td.get('document')
                            if not docname:
                                continue
                            neigh = list(per_doc_neighbors.get(docname, []) or [])
                            agg: Dict[str, float] = {}
                            for n in neigh:
                                try:
                                    p = n.get('path')
                                    s = float(n.get('score') or 0.0)
                                    if p:
                                        agg[p] = max(agg.get(p, 0.0), s)
                                except Exception:
                                    continue
                            ranked = sorted(agg.items(), key=lambda kv: kv[1], reverse=True)[:top_k]
                            doc_entries[docname] = {"explanatory_db_images": {p: s for p, s in ranked}}
                        tmp_map[qname] = doc_entries
                    per_query_sim_checks_payload = tmp_map
                try_save_tif_per_query_to_db(
                    per_query_results=per_query_results,
                    pg_config=copy.deepcopy(cloned_config.get('results_postgresql', {}) or {}),
                    run_identifier=run_identifier,
                    requesting_username=requesting_username,
                    per_query_sim_checks=(None if ('sim_img_check' in removal_set) else per_query_sim_checks_payload),
                    per_query_timestamps=per_query_timestamps,
                    global_top_docs=(None if ('global_top_docs' in removal_set) else (global_top_docs_names if save_global_top_docs else [])),
                    extra_columns=ke_columns,
                    per_query_extra_values=per_query_extra_values,
                    remove_columns_from_results=removal_list,
                )
        except Exception as e_dbg:
            logger.warning("Failed to save per-query results to PostgreSQL (augmented): %s", e_dbg)

        # Debug CSV export for per-query results (augmented path)
        try:
            pg_config_dbg = copy.deepcopy(cloned_config.get('results_postgresql', {}) or {})
            if pg_config_dbg.get('debug_export_csv', False) and save_outputs_to_folder:
                key_enrichment = cloned_config.get('key_enrichment') or {}
                ke_columns: List[str] = list(key_enrichment.get('columns_for_results') or [])
                per_query_extra_values: Dict[str, Any] = key_enrichment.get('per_query_metadata_map') or {}
                # Build per-query sim checks map only if not removed (allowed in privacy mode; DB-only)
                per_query_sim_checks_map_dbg: Optional[Dict[str, Any]] = None
                if ('sim_img_check' not in removal_set):
                    tmp_map: Dict[str, Any] = {}
                    for pq in per_query_results:
                        qname = pq.get('query_document')
                        if not qname:
                            continue
                        doc_entries: Dict[str, Any] = {}
                        for td in (pq.get('top_docs') or []):
                            docname = td.get('document')
                            if not docname:
                                continue
                            neigh = list(per_doc_neighbors.get(docname, []) or [])
                            agg: Dict[str, float] = {}
                            for n in neigh:
                                try:
                                    p = n.get('path')
                                    s = float(n.get('score') or 0.0)
                                    if p:
                                        agg[p] = max(agg.get(p, 0.0), s)
                                except Exception:
                                    continue
                            ranked = sorted(agg.items(), key=lambda kv: kv[1], reverse=True)[:top_k]
                            doc_entries[docname] = {"explanatory_db_images": {p: s for p, s in ranked}}
                        tmp_map[qname] = doc_entries
                    per_query_sim_checks_map_dbg = tmp_map
                write_tif_per_query_results_csv(
                    per_query_results=per_query_results,
                    export_dir=run_root,
                    run_id=run_identifier,
                    user=requesting_username,
                    extra_columns=ke_columns,
                    per_query_extra_values=per_query_extra_values,
                    per_query_sim_checks=(per_query_sim_checks_map_dbg if ('sim_img_check' not in removal_set) else None),
                    per_query_timestamps=per_query_timestamps,
                    filename_stem='postgres_export_tif_per_query',
                    global_top_docs=(None if ('global_top_docs' in removal_set) else (global_top_docs_names if save_global_top_docs else [])),
                    remove_columns=removal_list,
                )
        except Exception as e_dbg:
            logger.warning("Failed to write per-query debug CSV (augmented): %s", e_dbg)

        return {
            "status": "success",
            "exit_code": 0,
            "message": f"Computed top {len(final_top_documents)} documents (global) with augmented search.",
            "top_documents": final_top_documents,
            "per_query": per_query_results,
            "indexed_image_count": indexed_item_count,
            "index_path": final_index_file_path_str,
            "config_was_modified": False,
            "fallback_to_bruteforce_message": fallback_to_bruteforce_message,
            "tif_run_output_path": str(run_root.resolve()) if save_outputs_to_folder else None,
            "tif_run_summary_json_path": str(run_summary_path.resolve()) if (save_outputs_to_folder and run_summary_path) else None,
            "tif_run_db_export_csv_path": tif_run_db_export_csv_path,
        }
    except Exception as e:
        logger.error("Augmented TIF batch search failed: %s", e, exc_info=True)
        return {"status": "error", "exit_code": 1, "message": f"Augmented search failed: {e}"}


def _build_image_index_from_folder(config: Dict[str, Any], project_root: Path) -> Dict[str, Any]:
    """Minimal image-index builder used by TIF indexing pipeline.

    Builds/updates a vector index from the configured image folder based on provider.
    Providers: faiss (default) | qdrant | bruteforce (no index build).
    """
    try:
        fe_conf = config.get('feature_extractor', {})
        feature_extractor = FeatureExtractor(config=fe_conf, project_root_path=project_root)

        db_conf = config.get('vector_database', {}) or {}
        provider = str(db_conf.get('provider', 'faiss')).lower()
        allow_fallback = bool(db_conf.get('allow_fallback', True))  # kept for logs
        if provider == 'qdrant' and QdrantManager is None:
            logger.warning("Qdrant DB is not implemented or not available; falling back to 'faiss' for index build.")
            provider = 'faiss'
        logger.info("Vector DB Build: provider=%s, allow_fallback=%s", provider, allow_fallback)

        idx_params = config.get('indexing_task', {}) or {}
        img_folder = _resolve_path(idx_params.get('image_folder_to_index'), project_root)
        if provider != 'bruteforce':
            if not (img_folder and img_folder.is_dir()):
                raise FileNotFoundError(f"Indexing folder is not a valid directory: {img_folder}")

        if provider == 'faiss':
            faiss_conf = db_conf.get('faiss', {}) or {}
            vdb = FaissIndexManager(
                feature_dim=feature_extractor.feature_dim,
                output_directory=faiss_conf.get('output_directory'),
                filename_stem=faiss_conf.get('filename_stem', 'faiss_index'),
                index_type=faiss_conf.get('index_type', 'flat'),
                model_name=feature_extractor.model_name,
                faiss_config=faiss_conf,
                project_root_path=project_root,
            )

            logger.info("Building FAISS index (type=%s) at output_directory=%s, filename_stem=%s",
                        faiss_conf.get('index_type', 'flat'), faiss_conf.get('output_directory'), faiss_conf.get('filename_stem', 'faiss_index'))

            exit_code_build = vdb.build_index_from_folder(
                feature_extractor=feature_extractor,
                image_folder=str(img_folder),
                batch_size=idx_params.get('batch_size', 32),
                force_rebuild=idx_params.get('force_rebuild_index', False),
                scan_subfolders=idx_params.get('scan_for_database_subfolders', False),
                ivf_train_samples_ratio=idx_params.get('ivf_train_samples_ratio', 0.1),
                ivf_train_samples_max=idx_params.get('ivf_train_samples_max', 50000),
            )
            return {
                "status": "success" if exit_code_build == 0 else "error",
                "exit_code": int(exit_code_build),
                "message": ("Index built successfully." if exit_code_build == 0 else "Index building failed."),
                "indexed_image_count": vdb.get_total_indexed_items(),
                "index_path": (
                    f"FAISS sharded index: {len(vdb.index_identifiers)} shards (first: {vdb.index_identifiers[0]})"
                    if hasattr(vdb, 'index_identifiers') and len(getattr(vdb, 'index_identifiers', [])) > 1
                    else str(vdb.index_path.resolve())
                ),
            }

        if provider == 'qdrant':
            qdrant_conf = db_conf.get('qdrant', {}) or {}
            collection_stem = qdrant_conf.get('collection_name_stem', 'image_similarity_collection')
            vdb = QdrantManager(
                feature_dim=feature_extractor.feature_dim,
                collection_name_stem=collection_stem,
                model_name=feature_extractor.model_name,
                qdrant_config=qdrant_conf,
                project_root_path=project_root,
            )

            # Log Qdrant operational mode and settings
            if qdrant_conf.get('location'):
                loc = qdrant_conf.get('location')
                mode_desc = f"embedded:{loc}"
            else:
                host = qdrant_conf.get('host', 'localhost')
                port = qdrant_conf.get('port', 6333)
                mode_desc = f"server:{host}:{port}"
            logger.info(
                "Building Qdrant index: mode=%s, collection=%s, distance=%s, quantization=%s, on_disk_hnsw=%s",
                mode_desc,
                vdb.collection_name,
                qdrant_conf.get('distance_metric', 'Cosine'),
                bool(qdrant_conf.get('enable_quantization', False)),
                bool(qdrant_conf.get('on_disk_hnsw_indexing', True)),
            )

            if bool(qdrant_conf.get('force_recreate_collection', False)):
                logger.info("Qdrant: force_recreate_collection=True; clearing existing collection before build.")
                vdb.clear_index()

            exit_code_build = vdb.build_index_from_folder(
                feature_extractor=feature_extractor,
                image_folder=str(img_folder),
                batch_size=idx_params.get('batch_size', 32),
                force_rebuild=False,  # handled by clear_index above
                scan_subfolders=idx_params.get('scan_for_database_subfolders', False),
            )
            status = "success" if exit_code_build == 0 else "error"
            total = vdb.get_total_indexed_items()
            logger.info("Qdrant collection '%s' now contains %d item(s).", vdb.collection_name, total)
            return {
                "status": status,
                "exit_code": int(exit_code_build),
                "message": ("Index built successfully." if exit_code_build == 0 else "Index building failed."),
                "indexed_image_count": total,
                "index_path": (
                    f"Qdrant sharded collections: {len(vdb.collection_names)} (first: {vdb.collection_names[0]})"
                    if hasattr(vdb, 'collection_names') and len(getattr(vdb, 'collection_names', [])) > 1
                    else f"qdrant:{vdb.collection_name}"
                ),
            }

        if provider == 'bruteforce':
            logger.info("Vector DB Build: provider='bruteforce' selected; skipping index build and relying on image DB folder.")
            return {
                "status": "success",
                "exit_code": 0,
                "message": "Bruteforce provider selected; no index built.",
                "indexed_image_count": None,
                "index_path": None,
            }

        # Unknown provider
        return {
            "status": "error",
            "exit_code": 1,
            "message": f"Unsupported vector_database.provider '{provider}'. Use faiss|qdrant|bruteforce.",
        }
    except Exception as e_build:
        logger.error("An exception occurred during the index building task: %s", e_build, exc_info=True)
        return {"status": "error", "exit_code": 1, "message": f"Index building failed: {e_build}"}


def build_index_from_tif_folder_workflow(config: Dict[str, Any], project_root: Path, input_tif_folder_override: Optional[Path] = None) -> Dict[str, Any]:
    """
    Orchestrates the TIF extraction and FAISS indexing pipeline.

    - Find OCR-matched pages via TifTextSearcher
    - Extract photos (YOLO or BBox)
    - Save crops into the DB image folder
    - Build FAISS index from that folder
    """
    if not TifTextSearcher or not PhotoExtractor:
        logger.error("TIF processing libraries are not available. Cannot proceed.")
        return {
            "status": "error", "exit_code": 1,
            "message": "TIF processing libraries are not available. Install 'tif_searcher' and 'photo_extractor'."
        }

    logger.info("--- Starting TIF to Image Indexing Workflow ---")
    
    source_folder = input_tif_folder_override or _resolve_path(config.get('indexing_task', {}).get('input_tif_folder_for_indexing'), project_root)
    db_image_folder = _resolve_path(config.get('indexing_task', {}).get('image_folder_to_index'), project_root)
    
    logger.info(f"Source TIF folder: {source_folder}")
    logger.info(f"Output database image folder: {db_image_folder}")
    
    if not source_folder or not source_folder.exists():
        msg = f"Input TIF folder not found: {source_folder}"
        logger.error(msg)
        return {"status": "error", "exit_code": 1, "message": msg}

    db_image_folder.mkdir(parents=True, exist_ok=True)

    tif_files = list(source_folder.glob("*.tif")) + list(source_folder.glob("*.tiff"))
    if not tif_files:
        msg = f"No .tif files found in {source_folder}. Exiting."
        logger.warning(msg)
        return {"status": "success", "exit_code": 0, "message": msg, "extracted_images_count": 0}

    try:
        searcher_conf = config.get('searcher_config') or {}
        tif_searcher_kwargs = searcher_conf if isinstance(searcher_conf, dict) and len(searcher_conf) > 0 else None
        searcher = TifTextSearcher(**tif_searcher_kwargs) if tif_searcher_kwargs else TifTextSearcher()

        pe_conf = config.get('photo_extractor_config', {}) or {}
        photo_extraction_mode = str(pe_conf.get('photo_extraction_mode', 'bbox')).lower()
        bbox_conf = pe_conf.get('bbox_extraction', {}) or {}
        bbox_list = bbox_conf.get('bbox_list', [])
        bbox_format = str(bbox_conf.get('bbox_format', 'xyxy'))
        bbox_normalized = bool(bbox_conf.get('normalized', False))

        if photo_extraction_mode == 'bbox':
            pe_conf_override = {'photo_extraction_mode': 'bbox', 'bbox_extraction': bbox_conf}
            logger.info("Workflow: initializing PhotoExtractor with mode=bbox; passing bbox_extraction only (no yolo_object_detection).")
            extractor = PhotoExtractor(config_override=pe_conf_override)
        else:
            logger.info("Workflow: initializing PhotoExtractor with mode=yolo; passing full photo_extractor_config (includes yolo_object_detection and inference settings).")
            extractor = PhotoExtractor(config_override=pe_conf)
        pe_debug = bool(pe_conf.get('photo_extractor_debug', False))
    except Exception as e:
        logger.critical("Failed to initialize TIF processing components: %s", e, exc_info=True)
        return {"status": "error", "exit_code": 1, "message": f"Failed to initialize TIF components: {e}"}

    per_doc_crops_map: Dict[str, int] = {}
    extracted_count = 0
    logger.info(f"Found {len(tif_files)} TIF files to process.")
    for tif_path in tqdm.tqdm(tif_files, desc="Extracting Images from TIFs"):
        try:
            per_doc_extracted = 0
            # 1. Use TifTextSearcher to find pages containing the target text.
            page_numbers = searcher.find_text_pages(tif_path)
            if not page_numbers:
                logger.info("Skipping %s: no OCR-matched pages.", tif_path.name)
                continue
            
            # 2. Use PhotoExtractor to get cropped images from those pages.
            for page_num in page_numbers:
                if photo_extraction_mode == 'yolo':
                    photos = extractor.extract_photos(tif_path, page_num)
                else:
                    with Image.open(tif_path) as _tif_img:
                        if not (1 <= page_num <= _tif_img.n_frames):
                            continue
                        _tif_img.seek(page_num - 1)
                        page_image = _tif_img.convert("RGB")
                        photos = extractor.extract_photos_from_bboxes(
                            page_image,
                            bboxes=bbox_list,
                            bbox_format=bbox_format,
                            normalized=bbox_normalized,
                        )
                        # Fallback: if user-provided bboxes produce zero crops, default to full-page crop
                        if not photos:
                            logger.warning("%s p%d: 0 bbox crops. Falling back to full-page crop.", tif_path.name, page_num)
                            photos = extractor.extract_photos_from_bboxes(
                                page_image,
                                bboxes=[[0.0, 0.0, 1.0, 1.0]],
                                bbox_format='xyxy',
                                normalized=True,
                            )
                n_crops = len(photos) if photos else 0
                per_doc_extracted += n_crops
                if 'pe_debug' in locals() and pe_debug:
                    logger.info("%s p%d: crops=%d", tif_path.name, page_num, n_crops)
                    print(f"CONSOLE DEBUG: {tif_path.name} p{page_num}: crops={n_crops}")
                per_doc_crops_map[tif_path.name] = per_doc_extracted
                for i, photo_pil_image in enumerate(photos):
                    unique_name = f"{tif_path.stem}_page{page_num}_photo{i}.jpg"
                    save_path = db_image_folder / unique_name
                    photo_pil_image.save(save_path, "JPEG")
                    extracted_count += 1
        except Exception as e:
            logger.error(f"Failed to process {tif_path}: {e}", exc_info=True)

    if 'pe_debug' in locals() and pe_debug:
        try:
            logger.info("PhotoExtractor Debug: Per-document crop counts:")
            for doc_name, cnt in per_doc_crops_map.items():
                logger.info("  %s: %d", doc_name, cnt)
                print(f"CONSOLE DEBUG: {doc_name} total crops={cnt}")
            logger.info("PhotoExtractor Debug: Total crops across all documents: %d", extracted_count)
            print(f"CONSOLE DEBUG: Total crops across all documents: {extracted_count}")
        except Exception:
            pass
    logger.info("Image extraction complete. Proceeding to build vector index.")
    
    index_result = _build_image_index_from_folder(config, project_root)

    final_msg = f"TIF extraction complete. Extracted images: {extracted_count}. " + index_result.get('message', '')
    result = {
        "status": index_result.get("status", "success"),
        "exit_code": index_result.get("exit_code", 0),
        "message": final_msg,
        "indexed_image_count": index_result.get("indexed_image_count"),
        "index_path": index_result.get("index_path"),
        "extracted_images_count": extracted_count
    }
    logger.info("--- TIF to Image Indexing Workflow Finished ---")
    return result


def execute_tif_batch_search_workflow(
    config: Dict[str, Any],
    project_root: Path,
    input_folder_override: Optional[Path] = None,
    top_doc_override: Optional[int] = None
) -> Dict[str, Any]:
    """Run the legacy TIF batch search and produce per-query and global results.

    Behavior
    - Per TIF: Find OCR-matched pages (TifTextSearcher), extract photos (YOLO/BBox), search top_k neighbors via the
      configured vector DB provider (FAISS|Qdrant) or, when explicitly configured and security allows,
      a brute-force folder. Aggregate results per parent document using aggregation_strategy ('max'|'sum'|'mean') and
      keep top_doc.
    - Global: Build a run-level "top_documents" list using max aggregation across queries.

    Extended outputs (feature parity with augmented path)
    - image_authenticity (dict[str, str]): A map of stable photo IDs ("<tif_stem>_q_<idx>.jpg") to predicted class
      names when the optional authenticity classifier is enabled and available; otherwise an empty dict {}.
    - fraud_doc_probability (str): "Very_High" | "High" | "No" based on:
        similar_doc_flag_threshold = YAML photo_authenticity_classifier_config.similar_doc_flag_threshold
                                      or constants.SIMILAR_DOC_FLAG_THRESHOLD
        Very_High if any top_docs score >= similar_doc_flag_threshold AND any photo class is among
                    non_authentic_image_classes
        High      if either condition holds
        No        otherwise
    - threshold_match_count (int): Count of items in per-query top_docs with score >= constants.SIMILAR_DOC_FLAG_THRESHOLD.

    Column removal semantics
    - removal_set = set(search_task.remove_columns_from_results or constants.DEFAULT_REMOVE_COLUMNS_FROM_RESULTS)
      This set suppresses columns in both per-query CSV and per-query sections in the run summary JSON. If
      'top_similar_docs' is present, the internal 'top_docs' field is also removed from per-query JSON.
    - Run-level JSON includes 'global_top_docs' only when search_task.save_global_top_docs is true and
      'global_top_docs' is not in removal_set.
    - DB save payloads for optional fields continue to be passed as None; threshold_match_count is CSV-only.

    Privacy gating
    - When search_task.privacy_mode is true, the workflow disables image copying, preview generation, and
      sim_img_check payload construction. Authenticity inference remains in-memory and does not persist crops.

    Parameters
    - config: Full search configuration (YAML-derived dict)
    - project_root: Absolute project root
    - input_folder_override: Optional override for search_task.input_tif_folder_for_search
    - top_doc_override: Optional override for search_task.top_doc

    Returns
    - Dict with keys: status, exit_code, message, top_documents, per_query, indexed_image_count, index_path,
      config_was_modified, fallback_to_bruteforce_message, tif_run_output_path (optional)
    """
    if not TifTextSearcher or not PhotoExtractor:
        logger.error("TIF processing libraries are not available. Cannot proceed.")
        return {
            "status": "error",
            "exit_code": 1,
            "message": "TIF processing libraries are not available. Install 'tif_searcher' and 'photo_extractor'."
        }

    logger.info("--- Starting TIF Batch Similarity Search Workflow ---")

    cloned_config = copy.deepcopy(config)
    search_conf = cloned_config.get("search_task", {})
    generate_previews = bool(search_conf.get('generate_tif_previews', True))
    create_per_query_subfolders = bool(search_conf.get('create_per_query_subfolders_for_tif', True))

    # Privacy-mode suppression in legacy path
    privacy_mode_enabled = bool(search_conf.get('privacy_mode', True))
    persist_query_crops_flag = not privacy_mode_enabled
    if privacy_mode_enabled:
        # Disable preview/copy to avoid writing or exposing images. Keep sim_img_check if explicitly enabled.
        generate_previews = False
        search_conf['copy_query_image_to_output'] = False
        search_conf['copy_similar_images_to_output'] = False
    
    if input_folder_override:
        search_conf["input_tif_folder_for_search"] = str(input_folder_override)
    if top_doc_override is not None:
        search_conf["top_doc"] = int(top_doc_override)

    # Resolve input folder
    input_folder = _resolve_path(search_conf.get("input_tif_folder_for_search"), project_root)
    if not input_folder or not input_folder.exists():
        return {"status": "error", "exit_code": 1, "message": "Input TIF folder for search not found or not provided."}

    # Prepare run output folder honoring flags
    base_output_folder = _resolve_path(search_conf.get('output_folder_for_results'), project_root)
    if not base_output_folder:
        return {"status": "error", "exit_code": 1, "message": "Search 'output_folder_for_results' is not configured."}
    create_new_subfolder = bool(search_conf.get('new_query_new_subfolder', True))
    run_identifier = datetime.now().strftime('%Y%m%d_%H%M%S')
    requesting_username = detect_requesting_username()
    run_type = "tif_doc_runs"
    # Global gate for saving outputs to filesystem
    save_outputs_to_folder = bool(search_conf.get('save_outputs_to_folder', True))
    run_root = (base_output_folder / requesting_username / run_type / run_identifier) if create_new_subfolder else base_output_folder
    if save_outputs_to_folder:
        run_root.mkdir(parents=True, exist_ok=True)

    # Batch-first augmentation pathway
    if bool(search_conf.get('index_query_doc_images', False)):
        return _execute_tif_batch_search_with_augmentation(
            config=cloned_config,
            project_root=project_root,
            input_folder=input_folder,
            base_output_folder=base_output_folder,
            run_root=run_root,
            run_identifier=run_identifier,
            requesting_username=requesting_username
        )

    # Optional photo authenticity classifier configuration (default-off)
    pac = cloned_config.get('photo_authenticity_classifier_config') or {}
    check_auth = bool(pac.get('check_image_authenticity', False))
    non_auth_classes = list(pac.get('non_authentic_image_classes', []) or [])
    try:
        similar_doc_flag_threshold = float(pac.get('similar_doc_flag_threshold', SIMILAR_DOC_FLAG_THRESHOLD))
    except Exception:
        similar_doc_flag_threshold = SIMILAR_DOC_FLAG_THRESHOLD
    classifier_cfg_path = pac.get('classifier_config_path')

    classifier = None
    classifier_initialized = False
    if check_auth:
        cfg_path_resolved = _resolve_path(classifier_cfg_path, project_root) if classifier_cfg_path else None
        classifier = try_init_classifier(str(cfg_path_resolved) if cfg_path_resolved else None)
        classifier_initialized = bool(classifier)

    # Parameters
    top_k = int(search_conf.get("top_k", 5))
    top_doc = int(search_conf.get("top_doc", 7))
    top_doc_global = int(search_conf.get("top_doc_global", top_doc))
    aggregation_strategy = str(search_conf.get("aggregation_strategy", "max")).lower()
    if aggregation_strategy not in {"max", "sum", "mean"}:
        logger.warning("Unsupported aggregation_strategy '%s'. Falling back to 'max'.", aggregation_strategy)
        aggregation_strategy = "max"

    # Initialize Feature Extractor
    try:
        feature_extractor = FeatureExtractor(cloned_config.get('feature_extractor', {}), project_root)
    except Exception as e_fe:
        logger.critical("Failed to initialize FeatureExtractor: %s", e_fe, exc_info=True)
        return {"status": "error", "exit_code": 1, "message": f"Initialization failed: {e_fe}"}

    # Initialize vector DB provider and fallback handling
    db_conf = cloned_config.get('vector_database', {}) or {}
    provider = str(db_conf.get('provider', 'faiss')).lower()
    faiss_conf = db_conf.get('faiss', {}) or {}
    qdrant_conf = db_conf.get('qdrant', {}) or {}
    allow_fallback = bool(db_conf.get('allow_fallback', True))
    if provider == 'qdrant' and QdrantManager is None:
        logger.warning("Qdrant DB is not implemented or not available; falling back to 'faiss' for search.")
        provider = 'faiss'

    vector_db_manager: Optional[Any] = None
    final_index_file_path_str: Optional[str] = None
    indexed_item_count: int = 0
    fallback_to_bruteforce_message: Optional[str] = None

    # Fallback selection for legacy path
    fallback_choice: str = str(db_conf.get('fallback_choice', 'transient')).lower()
    if fallback_choice not in {"transient", "bruteforce"}:
        logger.warning("Unsupported fallback_choice '%s'. Defaulting to 'transient'.", fallback_choice)
        fallback_choice = "transient"
    selected_fallback_mode: Optional[str] = None  # 'transient' | 'bruteforce' | None

    logger.info("Vector DB Search: provider=%s, allow_fallback=%s, fallback_choice=%s", provider, allow_fallback, fallback_choice)

    if provider == 'faiss':
        try:
            vector_db_manager = FaissIndexManager(
                feature_dim=feature_extractor.feature_dim,
                output_directory=faiss_conf.get('output_directory'),
                filename_stem=faiss_conf.get('filename_stem', 'faiss_index'),
                index_type=faiss_conf.get('index_type', 'flat'),
                model_name=feature_extractor.model_name,
                faiss_config=faiss_conf,
                project_root_path=project_root
            )
            if not vector_db_manager.load_index():
                if allow_fallback:
                    # Choose fallback behavior based on configuration
                    if fallback_choice == 'bruteforce':
                        fallback_to_bruteforce_message = (
                            "Could not load the 'faiss' index. Brute-force fallback will be used for this run."
                        )
                        selected_fallback_mode = 'bruteforce'
                    else:
                        fallback_to_bruteforce_message = (
                            "Could not load the 'faiss' index. Transient-only search will be used for this run."
                        )
                        selected_fallback_mode = 'transient'
                    logger.warning(fallback_to_bruteforce_message)
                    vector_db_manager = None
                else:
                    return {
                        "status": "error",
                        "exit_code": 1,
                        "message": "FAISS index could not be loaded and fallback is disabled. Please build the index or enable fallback.",
                    }
            else:
                if vector_db_manager.is_index_loaded_and_ready():
                    indexed_item_count = vector_db_manager.get_total_indexed_items()
                    final_index_file_path_str = (
                        f"FAISS sharded index: {len(vector_db_manager.index_identifiers)} shards (first: {vector_db_manager.index_identifiers[0]})"
                        if hasattr(vector_db_manager, 'index_identifiers') and len(getattr(vector_db_manager, 'index_identifiers', [])) > 1
                        else str(vector_db_manager.index_path.resolve())
                    )
                    logger.info("Vector database (FAISS) loaded successfully with %d items.", indexed_item_count)
                else:
                    if allow_fallback:
                        if fallback_choice == 'bruteforce':
                            fallback_to_bruteforce_message = "FAISS index is not ready. Brute-force fallback will be used for this run."
                            selected_fallback_mode = 'bruteforce'
                        else:
                            fallback_to_bruteforce_message = "FAISS index is not ready. Transient-only search will be used for this run."
                            selected_fallback_mode = 'transient'
                        logger.warning(fallback_to_bruteforce_message)
                        vector_db_manager = None
                    else:
                        return {"status": "error", "exit_code": 1, "message": "FAISS index is not ready and fallback is disabled."}
        except Exception as e_init:
            logger.error("Error initializing FAISS manager: %s", e_init, exc_info=True)
            if allow_fallback:
                if fallback_choice == 'bruteforce':
                    fallback_to_bruteforce_message = "Error initializing FAISS manager. Brute-force fallback will be used for this run."
                    selected_fallback_mode = 'bruteforce'
                else:
                    fallback_to_bruteforce_message = "Error initializing FAISS manager. Transient-only search will be used for this run."
                    selected_fallback_mode = 'transient'
                vector_db_manager = None
            else:
                return {"status": "error", "exit_code": 1, "message": f"Initialization failed: {e_init}"}

    elif provider == 'qdrant':
        try:
            collection_stem = qdrant_conf.get('collection_name_stem', 'image_similarity_collection')
            vdb = QdrantManager(
                feature_dim=feature_extractor.feature_dim,
                collection_name_stem=collection_stem,
                model_name=feature_extractor.model_name,
                qdrant_config=qdrant_conf,
                project_root_path=project_root,
            )
            vdb_ready = vdb.load_index() and vdb.is_index_loaded_and_ready()

            # Build descriptive index identifier for logs
            if qdrant_conf.get('location'):
                loc = qdrant_conf.get('location')
                mode_desc = f"embedded:{loc}"
            else:
                host = qdrant_conf.get('host', 'localhost')
                port = qdrant_conf.get('port', 6333)
                mode_desc = f"server:{host}:{port}"
            final_index_file_path_str = f"qdrant:{vdb.collection_name} ({mode_desc})"

            if vdb_ready:
                vector_db_manager = vdb
                indexed_item_count = vdb.get_total_indexed_items()
                logger.info(
                    "Vector database (Qdrant) ready: mode=%s, collection=%s, distance=%s, quantization=%s, on_disk_hnsw=%s, items=%d",
                    mode_desc,
                    vdb.collection_name,
                    qdrant_conf.get('distance_metric', 'Cosine'),
                    bool(qdrant_conf.get('enable_quantization', False)),
                    bool(qdrant_conf.get('on_disk_hnsw_indexing', True)),
                    indexed_item_count,
                )
            else:
                if allow_fallback:
                    if fallback_choice == 'bruteforce':
                        fallback_to_bruteforce_message = (
                            "Qdrant collection is empty or not ready. Brute-force fallback will be used for this run."
                        )
                        selected_fallback_mode = 'bruteforce'
                    else:
                        fallback_to_bruteforce_message = (
                            "Qdrant collection is empty or not ready. Transient-only search will be used for this run."
                        )
                        selected_fallback_mode = 'transient'
                    logger.warning(fallback_to_bruteforce_message)
                    vector_db_manager = None
                else:
                    return {
                        "status": "error",
                        "exit_code": 1,
                        "message": "Qdrant index is not ready and fallback is disabled. Build the index or enable fallback.",
                    }
        except Exception as e_q:
            logger.error("Error initializing Qdrant manager: %s", e_q, exc_info=True)
            if allow_fallback:
                if fallback_choice == 'bruteforce':
                    fallback_to_bruteforce_message = "Error initializing Qdrant manager. Brute-force fallback will be used for this run."
                    selected_fallback_mode = 'bruteforce'
                else:
                    fallback_to_bruteforce_message = "Error initializing Qdrant manager. Transient-only search will be used for this run."
                    selected_fallback_mode = 'transient'
                vector_db_manager = None
            else:
                return {"status": "error", "exit_code": 1, "message": f"Initialization failed: {e_q}"}

    elif provider == 'bruteforce':
        vector_db_manager = None
        fallback_to_bruteforce_message = "Bruteforce provider selected; using direct folder scan (no index)."
        logger.info("Vector DB Search: provider='bruteforce' -> using brute-force database folder.")

    else:
        return {"status": "error", "exit_code": 1, "message": f"Unsupported vector_database.provider '{provider}'. Use faiss|qdrant|bruteforce."}

    # Privacy guardrail: disallow bruteforce when privacy_mode=true (no persistence)
    privacy_mode_enabled = bool(search_conf.get('privacy_mode', True))
    if privacy_mode_enabled and (provider == 'bruteforce' or (vector_db_manager is None)):
        return {
            'status': 'error',
            'exit_code': 1,
            'message': 'Security mode active (privacy_mode=true): bruteforce requires persisted crops. Cannot proceed. Configure provider=faiss or qdrant, or set privacy_mode=false.'
        }

    # Determine brute-force DB folder
    bruteforce_db_folder = search_conf.get('bruteforce_db_folder') or cloned_config.get('indexing_task', {}).get('image_folder_to_index')
    bruteforce_db_folder_path = _resolve_path(bruteforce_db_folder, project_root) if bruteforce_db_folder else None

    # Initialize searcher with explicit fallback_mode based on selected_fallback_mode
    searcher = ImageSimilaritySearcher(
        feature_extractor,
        vector_db_manager,
        fallback_mode=selected_fallback_mode
    )

    # Initialize TIF processing components (with optional searcher_config)
    try:
        searcher_conf = cloned_config.get('searcher_config') or {}
        tif_searcher_kwargs = searcher_conf if isinstance(searcher_conf, dict) and len(searcher_conf) > 0 else None
        tif_searcher = TifTextSearcher(**tif_searcher_kwargs) if tif_searcher_kwargs else TifTextSearcher()

        pe_conf = cloned_config.get('photo_extractor_config', {}) or {}
        photo_extraction_mode = str(pe_conf.get('photo_extraction_mode', 'bbox')).lower()
        bbox_conf = pe_conf.get('bbox_extraction', {}) or {}
        bbox_list = bbox_conf.get('bbox_list', [])
        bbox_format = str(bbox_conf.get('bbox_format', 'xyxy'))
        bbox_normalized = bool(bbox_conf.get('normalized', False))
        if photo_extraction_mode == 'bbox':
            pe_conf_override = {'photo_extraction_mode': 'bbox', 'bbox_extraction': bbox_conf}
            logger.info("Workflow: initializing PhotoExtractor with mode=bbox; passing bbox_extraction only (no yolo_object_detection).")
            photo_extractor = PhotoExtractor(config_override=pe_conf_override)
        else:
            logger.info("Workflow: initializing PhotoExtractor with mode=yolo; passing full photo_extractor_config (includes yolo_object_detection and inference settings).")
            photo_extractor = PhotoExtractor(config_override=pe_conf)
        pe_debug = bool(pe_conf.get('photo_extractor_debug', False))
    except Exception as e_tif:
        logger.critical("Failed to initialize TIF processing components: %s", e_tif, exc_info=True)
        return {"status": "error", "exit_code": 1, "message": f"Failed to initialize TIF components: {e_tif}"}

    # Collect TIF files
    query_tif_files = list(input_folder.glob("*.tif")) + list(input_folder.glob("*.tiff"))
    if not query_tif_files:
        msg = f"No query .tif files found in {input_folder}. Exiting."
        logger.warning(msg)
        return {
            "status": "success",
            "exit_code": 0,
            "message": msg,
            "top_documents": [],
            "per_query": [],
            "indexed_image_count": indexed_item_count,
            "index_path": final_index_file_path_str,
            "config_was_modified": False,
            "fallback_to_bruteforce_message": fallback_to_bruteforce_message
        }

    # Global aggregation (max per parent)
    document_scores: Dict[str, float] = {}
    per_query_results: List[Dict[str, Any]] = []
    per_doc_neighbors: Dict[str, List[Dict[str, Any]]] = {}
    per_doc_crops_map: Dict[str, int] = {}
    global_query_paths: List[Path] = []
    # Per-query CSV enrichment
    per_query_timestamps: Dict[str, str] = {}
    per_query_sim_checks_map: Dict[str, Any] = {}
    # Saved-name -> original-key-name (with .tif) mapping injected by key orchestrator
    name_map: Dict[str, str] = (cloned_config.get('key_input_runtime', {}) or {}).get('query_name_map', {}) or {}

    for tif_path in tqdm.tqdm(query_tif_files, desc="Batch Searching TIFs"):
        t_start = time.perf_counter()
        try:
            matched_name = tif_path.name
            original_name = name_map.get(matched_name, matched_name)
            doc_crop_count = 0
            # Collect neighbor evidence per parent for this query
            local_per_doc_neighbors: Dict[str, List[Dict[str, Any]]] = {}
            page_numbers = tif_searcher.find_text_pages(tif_path)
            if not page_numbers:
                logger.info("TIF '%s': no OCR-matched pages. Skipping.", tif_path.name)
                per_query_results.append({
                    "query_document": original_name,
                    "matched_query_document": matched_name,
                    "num_query_photos": 0,
                    "top_docs": [],
                    "elapsed_seconds": 0.0,
                    "top_k_used": top_k,
                    "top_doc_used": top_doc,
                    "aggregation_strategy_used": aggregation_strategy,
                    "threshold_match_count": 0,
                })
                continue

            logger.info("TIF '%s': matched %d page(s).", tif_path.name, len(page_numbers))

            # Extract all photos for this TIF
            query_photos: List[Image.Image] = []
            for page_num in page_numbers:
                if photo_extraction_mode == 'yolo':
                    photos = photo_extractor.extract_photos(tif_path, page_num)
                else:
                    with Image.open(tif_path) as _tif_img:
                        if not (1 <= page_num <= _tif_img.n_frames):
                            continue
                        _tif_img.seek(page_num - 1)
                        page_image = _tif_img.convert('RGB')
                        photos = photo_extractor.extract_photos_from_bboxes(
                            page_image,
                            bboxes=bbox_list,
                            bbox_format=bbox_format,
                            normalized=bbox_normalized,
                        )
                        if not photos:
                            logger.warning("%s p%d: 0 bbox crops. Falling back to full-page crop.", tif_path.name, page_num)
                            photos = photo_extractor.extract_photos_from_bboxes(
                                page_image,
                                bboxes=[[0.0, 0.0, 1.0, 1.0]],
                                bbox_format='xyxy',
                                normalized=True,
                            )
                n_crops = len(photos or [])
                doc_crop_count += n_crops
                per_doc_crops_map[tif_path.name] = doc_crop_count
                if 'pe_debug' in locals() and pe_debug:
                    logger.info("TIF '%s' p%d: crops=%d", tif_path.name, page_num, n_crops)
                    print(f"CONSOLE DEBUG: {tif_path.name} p{page_num}: crops={n_crops}")
                query_photos.extend(photos or [])

            logger.info("TIF '%s': extracted %d photo(s).", tif_path.name, len(query_photos))
            if 'pe_debug' in locals() and pe_debug:
                print(f"CONSOLE DEBUG: TIF '{tif_path.name}': extracted {len(query_photos)} photo(s).")

            # Optional authenticity classification per cropped photo (guarded)
            image_auth_map_local: Dict[str, str] = {}
            has_non_authentic_photo: bool = False
            if check_auth and classifier is not None:
                try:
                    for idx, photo in enumerate(query_photos):
                        try:
                            result = classifier.infer(photo)
                            detected_class = str((result or {}).get('class_name') or 'unknown')
                        except Exception:
                            detected_class = 'unknown'
                        stable_photo_id = f"{tif_path.stem}_q_{idx}.jpg"
                        image_auth_map_local[stable_photo_id] = detected_class
                        if detected_class in non_auth_classes:
                            has_non_authentic_photo = True
                except Exception as e_auth:
                    logger.warning("Auth classifier inference failed for '%s': %s. Skipping authenticity checks for this query.", tif_path.name, e_auth)
                    image_auth_map_local = {}
                    has_non_authentic_photo = False
            else:
                image_auth_map_local = {}
                has_non_authentic_photo = False

            per_doc_scores: Dict[str, List[float]] = {}

            if not vector_db_manager and not bruteforce_db_folder_path:
                logger.error("Brute-force DB folder not configured. Set search_task.bruteforce_db_folder or indexing_task.image_folder_to_index.")
                raise RuntimeError("Brute-force DB folder not configured for fallback search.")

            temp_dir: Optional[tempfile.TemporaryDirectory] = None
            try:
                temp_dir = tempfile.TemporaryDirectory(prefix="tif_query_photos_")
                for idx, photo in enumerate(query_photos):
                    temp_query_path = Path(temp_dir.name) / f"{tif_path.stem}_q_{idx}.jpg"
                    try:
                        photo.convert('RGB').save(temp_query_path, "JPEG")
                    except Exception:
                        temp_query_path = temp_query_path.with_suffix('.png')
                        photo.convert('RGB').save(temp_query_path, "PNG")

                    results, method, _, _ = searcher.search_similar_images(
                        query_image_path=temp_query_path,
                        top_k=top_k,
                        db_folder_for_bruteforce=bruteforce_db_folder_path,
                        bruteforce_batch_size=int(search_conf.get('bruteforce_batch_size', 32)),
                        ivf_nprobe_search=faiss_conf.get('ivf_nprobe_search'),
                        hnsw_efsearch_search=faiss_conf.get('hnsw_efsearch_search'),
                    )

                    for similar_img_path, score in results:
                        stem = Path(similar_img_path).stem
                        parent_doc_name = parent_doc_from_db_stem(stem)
                        per_doc_scores.setdefault(parent_doc_name, []).append(float(score))
                        prev = document_scores.get(parent_doc_name)
                        if prev is None or score > prev:
                            document_scores[parent_doc_name] = float(score)
                        entry = {
                            "path": str(Path(similar_img_path).resolve()),
                            "score": float(score),
                            "query_photo_id": idx
                        }
                        per_doc_neighbors.setdefault(parent_doc_name, []).append(entry)
                        local_per_doc_neighbors.setdefault(parent_doc_name, []).append(entry)
            finally:
                if temp_dir is not None:
                    temp_dir.cleanup()

            # Aggregate per-query scores
            agg_scores: Dict[str, float] = {}
            for doc_name, scores in per_doc_scores.items():
                if not scores:
                    continue
                if aggregation_strategy == 'max':
                    agg_val = max(scores)
                elif aggregation_strategy == 'sum':
                    agg_val = float(np.sum(np.array(scores, dtype=float)))
                else:
                    agg_val = float(np.mean(np.array(scores, dtype=float)))
                agg_scores[doc_name] = float(agg_val)

            top_docs_for_query = [
                {"document": d, "score": s}
                for d, s in sorted(agg_scores.items(), key=lambda kv: kv[1], reverse=True)[:top_doc]
            ]

            # Compute fraud probability based on similarity and authenticity flags
            fraud_doc_probability = compute_fraud_probability(
                top_docs_for_query,
                non_auth_classes,
                image_auth_map_local,
                similar_doc_flag_threshold,
            )

            elapsed = time.perf_counter() - t_start
            logger.info(
                "TIF '%s': photos=%d, strategy=%s, top_k_used=%d, top_doc_used=%d, elapsed=%.3fs",
                tif_path.name, len(query_photos), aggregation_strategy, top_k, top_doc, elapsed
            )
            logger.info("TIF '%s': per-query top_docs: %s", tif_path.name, top_docs_for_query)
            # Record per-query timestamp
            per_query_timestamps[original_name] = datetime.now().isoformat()
            # Build per-query sim_img_check mapping
            if cloned_config.get('search_task', {}).get('doc_sim_img_check', False):
                max_k_cap = cloned_config.get('search_task', {}).get('doc_sim_img_check_max_k')
                _map: Dict[str, Any] = {}
                for td in top_docs_for_query:
                    docname = td.get('document')
                    neigh = list(local_per_doc_neighbors.get(docname, []) or [])
                    agg: Dict[str, float] = {}
                    for n in neigh:
                        try:
                            p = n.get('path')
                            s = float(n.get('score') or 0.0)
                            if p:
                                agg[p] = max(agg.get(p, 0.0), s)
                        except Exception:
                            continue
                    ranked = sorted(agg.items(), key=lambda kv: kv[1], reverse=True)
                    limit = int(max_k_cap) if max_k_cap else top_k
                    ranked = ranked[:limit]
                    _map[docname] = {"explanatory_db_images": [{"path": p, "score": s} for p, s in ranked]}
                per_query_sim_checks_map[original_name] = _map
            else:
                per_query_sim_checks_map[original_name] = {}

            # Attach key-enrichment metadata for this query if provided
            _ke = cloned_config.get('key_enrichment') or {}
            _ke_cols: List[str] = list((_ke.get('columns_for_results') or []))
            _ke_map: Dict[str, Any] = _ke.get('per_query_metadata_map') or {}
            _ke_values = None
            if _ke_cols and _ke_map:
                src_name = original_name
                row = _ke_map.get(src_name)
                if row:
                    _ke_values = {c: row.get(c) for c in _ke_cols}

            per_query_entry = {
                "query_document": original_name,
                "matched_query_document": matched_name,
                "num_query_photos": len(query_photos),
                "top_docs": top_docs_for_query,
                "elapsed_seconds": elapsed,
                "top_k_used": top_k,
                "top_doc_used": top_doc,
                "aggregation_strategy_used": aggregation_strategy,
                "key_metadata": _ke_values,
            }
            if check_auth and classifier_initialized:
                per_query_entry["image_authenticity"] = image_auth_map_local
                per_query_entry["fraud_doc_probability"] = fraud_doc_probability
            per_query_results.append(per_query_entry)

            # Copy artifacts for this query (query copy independent of similar-doc copies)
            per_query_folder = (run_root / tif_path.name) if create_per_query_subfolders else None

            # Always copy the query TIF if configured
            if save_outputs_to_folder and search_conf.get('copy_query_image_to_output', True):
                try:
                    base_dir_for_query_copy = per_query_folder if create_per_query_subfolders else run_root
                    base_dir_for_query_copy.mkdir(parents=True, exist_ok=True)
                    if search_conf.get('save_query_in_separate_subfolder_if_copied', True):
                        q_dst_dir = base_dir_for_query_copy / "_query_image_source"
                        q_dst_dir.mkdir(exist_ok=True)
                    else:
                        q_dst_dir = base_dir_for_query_copy
                    shutil.copy2(tif_path, q_dst_dir / tif_path.name)
                    try:
                        global_query_paths.append(tif_path)
                    except Exception:
                        pass

                    # Generate preview JPG for query (first page)
                    if generate_previews:
                        try:
                            preview_path = q_dst_dir / f"{tif_path.stem}_preview.jpg"
                            generate_tif_preview(tif_path, preview_path)
                        except Exception as e_prev:
                            logger.warning("Failed to create preview for query TIF '%s': %s", tif_path, e_prev)
                except Exception as e_copy:
                    logger.warning("Failed to copy query TIF '%s' -> '%s': %s", tif_path, q_dst_dir, e_copy)

            # Copy parent TIFs ranked for this query if configured
            if save_outputs_to_folder and search_conf.get('copy_similar_images_to_output', True) and top_docs_for_query:
                if not create_per_query_subfolders:
                    logger.info("Per-query subfolders disabled. Skipping per-query ranked TIF copies. Global top_documents will be copied to run root.")
                else:
                    per_query_folder.mkdir(parents=True, exist_ok=True)
                    for rank, item in enumerate(top_docs_for_query, 1):
                        parent_name = item['document']
                        src = None
                        for candidate_root in [search_conf.get('input_tif_folder_for_search'), config.get('indexing_task', {}).get('input_tif_folder_for_indexing')]:
                            if not candidate_root:
                                continue
                            root_path = _resolve_path(candidate_root, project_root)
                            candidate_path = root_path / parent_name
                            if candidate_path.is_file():
                                src = candidate_path
                                break
                            if root_path and root_path.exists():
                                try:
                                    src = next(root_path.rglob(parent_name))
                                    break
                                except StopIteration:
                                    pass
                        if src is None:
                            logger.warning("Parent TIF '%s' not found in configured folders. Skipping copy.", parent_name)
                            continue
                        dest = per_query_folder / f"ranked_{rank:02d}_{Path(parent_name).name}"
                        try:
                            shutil.copy2(src, dest)
                            # Generate preview for ranked parent TIF
                            if generate_previews:
                                try:
                                    _base = dest.with_suffix('')
                                    preview_path = _base.with_name(_base.name + "_preview.jpg")
                                    generate_tif_preview(dest, preview_path)
                                except Exception as e_prev:
                                    logger.warning("Failed to create preview for ranked TIF '%s': %s", dest, e_prev)
                        except Exception as e_copy:
                            logger.warning("Failed to copy parent TIF '%s' -> '%s': %s", src, dest, e_copy)

            # Save per-query summary JSON inside its folder when enabled
            if save_outputs_to_folder and cloned_config.get('search_task', {}).get('save_search_summary_json', True) and per_query_folder:
                try:
                    per_query_folder.mkdir(parents=True, exist_ok=True)
                    per_query_summary = {
                        "global": {
                            "aggregation_strategy_used": aggregation_strategy,
                            "top_k_used": top_k,
                            "top_doc_used": top_doc,
                            "fallback_to_bruteforce_message": fallback_to_bruteforce_message,
                            "indexed_image_count": indexed_item_count,
                            "index_path": final_index_file_path_str,
                            "photo_authenticity_enabled": bool(check_auth and classifier_initialized),
                            "similar_doc_flag_threshold": similar_doc_flag_threshold,
                            "non_authentic_image_classes": non_auth_classes,
                        },
                        "per_query": [per_query_entry],
                    }
                    with open(per_query_folder / 'search_results_summary.json', 'w', encoding='utf-8') as f:
                        json.dump(per_query_summary, f, ensure_ascii=False, indent=2)
                except Exception as e_jsq:
                    logger.warning("Failed to write per-query summary for '%s': %s", tif_path.name, e_jsq)

        except Exception as e_q:
            logger.error("Could not process query TIF %s: %s", tif_path, e_q, exc_info=True)
            matched_name = tif_path.name
            original_name = name_map.get(matched_name, matched_name)
            per_query_results.append({
                "query_document": original_name,
                "matched_query_document": matched_name,
                "num_query_photos": 0,
                "top_docs": [],
                "elapsed_seconds": 0.0,
                "top_k_used": top_k,
                "top_doc_used": top_doc,
                "aggregation_strategy_used": aggregation_strategy,
                "error": str(e_q)
            })

    if 'pe_debug' in locals() and pe_debug:
        try:
            logger.info("PhotoExtractor Debug: Per-document crop counts (batch):")
            _total_crops_batch = 0
            for doc_name, cnt in per_doc_crops_map.items():
                logger.info("  %s: %d", doc_name, cnt)
                print(f"CONSOLE DEBUG: {doc_name} total crops={cnt}")
                _total_crops_batch += int(cnt)
            logger.info("PhotoExtractor Debug: Total crops across batch: %d", _total_crops_batch)
            print(f"CONSOLE DEBUG: Total crops across batch: {_total_crops_batch}")
        except Exception:
            pass
    
    # Final global ranking
    if not document_scores:
        msg = "No similar documents found across all queries."
        logger.info(msg)
        return {
            "status": "success",
            "exit_code": 0,
            "message": msg,
            "top_documents": [],
            "per_query": per_query_results,
            "indexed_image_count": indexed_item_count,
            "index_path": final_index_file_path_str,
            "config_was_modified": False,
            "fallback_to_bruteforce_message": fallback_to_bruteforce_message
        }

    final_top_documents = [
        {"document": doc, "score": score}
        for doc, score in sorted(document_scores.items(), key=lambda item: item[1], reverse=True)[:top_doc_global]
    ]
    # Optional global-top persistence controls
    save_global_top_docs = bool(cloned_config.get('search_task', {}).get('save_global_top_docs', False))
    global_top_docs_names: List[str] = [item['document'] for item in final_top_documents] if final_top_documents else []

    # Prepare consolidated JSON and disk outputs
    summary_filename = Path(cloned_config.get('search_task', {}).get('search_summary_json_filename', 'search_results_summary.json')).name
    run_summary_path: Optional[Path] = None

    # Column removal control: list of column names to hide from CSV and JSON payloads
    removal_list = list((cloned_config.get('search_task', {}) or {}).get('remove_columns_from_results', DEFAULT_REMOVE_COLUMNS_FROM_RESULTS) or [])
    removal_set = effective_removal_set(removal_list, DEFAULT_REMOVE_COLUMNS_FROM_RESULTS)

    def _filter_per_query_for_json(items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Filter per-query entries for JSON output according to removal_set.

        Args:
            items (List[Dict[str, Any]]): Raw per-query entries.

        Returns:
            List[Dict[str, Any]]: Filtered entries with keys removed per removal_set;
            also drops internal 'top_docs' when either 'top_similar_docs' or 'top_docs' is removed.
        """
        # Drop fields from per-query entries that correspond to removed columns
        filtered: List[Dict[str, Any]] = []
        for entry in items:
            e = dict(entry)
            if 'top_similar_docs' in removal_set or 'top_docs' in removal_set:
                e.pop('top_docs', None)
            if 'image_authenticity' in removal_set:
                e.pop('image_authenticity', None)
            if 'fraud_doc_probability' in removal_set:
                e.pop('fraud_doc_probability', None)
            if 'matched_query_document' in removal_set:
                e.pop('matched_query_document', None)
            if 'query_document' in removal_set:
                e.pop('query_document', None)
            filtered.append(e)
        return filtered

    if save_outputs_to_folder and cloned_config.get('search_task', {}).get('save_search_summary_json', True):
        global_section = {
            "aggregation_strategy_used": aggregation_strategy,
            "top_k_used": top_k,
            "top_doc_used": top_doc,
            "fallback_to_bruteforce_message": fallback_to_bruteforce_message,
            "indexed_image_count": indexed_item_count,
            "index_path": final_index_file_path_str,
            "photo_authenticity_enabled": bool(check_auth and classifier_initialized),
            "similar_doc_flag_threshold": similar_doc_flag_threshold,
            "non_authentic_image_classes": non_auth_classes,
        }
        # Optionally embed global top documents in summary JSON
        if save_global_top_docs and ('global_top_docs' not in removal_set):
            try:
                global_section["global_top_docs"] = global_top_docs_names
            except Exception:
                global_section["global_top_docs"] = []
        run_summary = {"global": global_section, "per_query": _filter_per_query_for_json(per_query_results)}
        run_summary_path = run_root / summary_filename
        try:
            with open(run_summary_path, 'w', encoding='utf-8') as f:
                json.dump(run_summary, f, ensure_ascii=False, indent=2)
            logger.info("Saved run summary JSON to '%s'", run_summary_path)
        except Exception as e_js:
            logger.error("Failed to write run summary JSON: %s", e_js)

    # Derive key-enrichment (optional) for extra columns in CSV/DB JSON payloads
    key_enrichment = cloned_config.get('key_enrichment') or {}
    ke_columns: List[str] = list(key_enrichment.get('columns_for_results') or [])
    ke_per_query_map: Dict[str, Dict[str, Any]] = key_enrichment.get('per_query_metadata_map') or {}

    # Map enrichment to parent_document_name using available neighbor evidence (paths contain filenames for db images)
    per_doc_extra_values: Dict[str, Dict[str, Any]] = {}
    if ke_columns and ke_per_query_map:
        try:
            # Prefer direct name match (TIF filenames saved into batch temp folder)
            for item in final_top_documents:
                parent_name = item['document']  # e.g., SomeDoc.tif
                extras = ke_per_query_map.get(parent_name)
                if not extras:
                    # Try neighbor file name mapping
                    neigh = per_doc_neighbors.get(parent_name)
                    if neigh:
                        for n in neigh:
                            try:
                                fname = Path(n.get('path', '')).name
                                extras = ke_per_query_map.get(fname)
                                if extras:
                                    break
                            except Exception:
                                continue
                if extras:
                    per_doc_extra_values[parent_name] = {col: extras.get(col) for col in ke_columns}
        except Exception as e_map:
            logger.warning("Failed to build key enrichment mapping for extra columns: %s", e_map)

    # Save to PostgreSQL if configured (global results only)
    tif_run_db_export_csv_path: Optional[str] = None
    if cloned_config.get('search_task', {}).get('save_results_to_postgresql', False):
        try:
            pg_config = copy.deepcopy(cloned_config.get('results_postgresql', {}) or {})
            # Build sim_img_check payload mapping based on flag
            if cloned_config.get('search_task', {}).get('doc_sim_img_check', False):
                max_k_cap = cloned_config.get('search_task', {}).get('doc_sim_img_check_max_k')
                sim_checks_map = build_sim_img_checks_map(final_top_documents, per_doc_neighbors, top_k, max_k_cap)
            else:
                sim_checks_map = {item['document']: {"note": "not required"} for item in final_top_documents}

            # Build aggregated per-query maps for DB (one per run)
            per_query_image_auth_map = { (entry.get('query_document')): (entry.get('image_authenticity') or {}) for entry in per_query_results }
            per_query_fraud_map = { (entry.get('query_document')): (entry.get('fraud_doc_probability') or 'No') for entry in per_query_results }

            logger.info("Skipping deprecated doc-level DB save; using per-query saving only.")

            # Optional: export per-query CSV when debug enabled
            if pg_config.get('debug_export_csv', False) and save_outputs_to_folder:
                try:
                    write_tif_per_query_results_csv(
                        per_query_results=per_query_results,
                        export_dir=run_root,
                        run_id=run_identifier,
                        user=requesting_username,
                        extra_columns=ke_columns,
                        per_query_extra_values=key_enrichment.get('per_query_metadata_map') or {},
                        per_query_sim_checks=(None if ('sim_img_check' in removal_set) else per_query_sim_checks_map),
                        per_query_timestamps=per_query_timestamps,
                        filename_stem="postgres_export_tif_per_query",
                        global_top_docs=(None if ('global_top_docs' in removal_set) else (global_top_docs_names if save_global_top_docs else [])),
                        remove_columns=removal_list,
                    )
                except Exception as e_csv2:
                    logger.warning("Failed to write per-query CSV: %s", e_csv2)
        except Exception as e_pg:
            logger.error("Failed to save TIF search results to PostgreSQL: %s", e_pg, exc_info=True)

    # Debug CSV export for per-query results only
    try:
        pg_config = copy.deepcopy(cloned_config.get('results_postgresql', {}) or {})
        if pg_config.get('debug_export_csv', False) and save_outputs_to_folder:
            write_tif_per_query_results_csv(
                per_query_results=per_query_results,
                export_dir=run_root,
                run_id=run_identifier,
                user=requesting_username,
                extra_columns=ke_columns,
                per_query_extra_values=key_enrichment.get('per_query_metadata_map') or {},
                per_query_sim_checks=(None if ('sim_img_check' in removal_set) else per_query_sim_checks_map),
                per_query_timestamps=per_query_timestamps,
                filename_stem="postgres_export_tif_per_query",
                global_top_docs=(None if ('global_top_docs' in removal_set) else (global_top_docs_names if save_global_top_docs else [])),
                remove_columns=removal_list,
            )
    except Exception as e_csv_dbg:
        logger.warning("Failed to write per-query debug CSV: %s", e_csv_dbg)

    logger.info("--- TIF Batch Similarity Search Workflow Finished ---")
    return {
        "status": "success",
        "exit_code": 0,
        "message": f"Computed top {len(final_top_documents)} documents (global).",
        "top_documents": final_top_documents,
        "per_query": per_query_results,
        "indexed_image_count": indexed_item_count,
        "index_path": final_index_file_path_str,
        "config_was_modified": False,
        "fallback_to_bruteforce_message": fallback_to_bruteforce_message,
        "tif_run_output_path": str(run_root.resolve()) if save_outputs_to_folder else None,
        "tif_run_summary_json_path": str(run_summary_path.resolve()) if (save_outputs_to_folder and run_summary_path) else None,
        "tif_run_db_export_csv_path": tif_run_db_export_csv_path
    }
