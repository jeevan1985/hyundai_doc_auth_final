{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter rows by image filename prefixes (batch streamed)\n",
    "\n",
    "This notebook loads a CSV / Excel / JSON table, optionally converts large Excel to CSV for streaming, then streams the rows in batches (using the same chunking logic pattern demonstrated in Key_Driven_TIF_Validation.ipynb) to filter rows whose COLUMN_NAME_TO_COMPARE matches any image filename prefix (filename[:N]) from a specified folder. Matching rows are written to a CSV in OUTPUT_FOLDER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adae3e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Generator, Iterable, Iterator, List, Optional, Sequence, Tuple\n",
    "import tempfile\n",
    "\n",
    "LOGGER = logging.getLogger('filter_key_val_data')\n",
    "if not LOGGER.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(logging.Formatter('%(asctime)s | %(levelname)s | %(name)s | %(message)s'))\n",
    "    LOGGER.addHandler(handler)\n",
    "LOGGER.setLevel(logging.INFO)\n",
    "\n",
    "def _ensure_iterable_chunks(iterable: Iterable[Any], chunk_size: int) -> Iterator[List[Any]]:\n",
    "    \"\"\"Yield successive lists (chunks) from an iterable, size up to chunk_size.\n",
    "    Mirrors the chunking logic approach used in the referenced notebook.\n",
    "    \"\"\"\n",
    "    chunk: List[Any] = []\n",
    "    cs = max(1, int(chunk_size))\n",
    "    for item in iterable:\n",
    "        chunk.append(item)\n",
    "        if len(chunk) >= cs:\n",
    "            yield chunk\n",
    "            chunk = []\n",
    "    if chunk:\n",
    "        yield chunk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f663cf09",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac158815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input table (CSV / Excel / JSON)\n",
    "INPUT_TABLE_PATH = Path(\"D:/mock_api_TEST/filtered_rows.xlsx\")  # change to your file path\n",
    "INPUT_FORMAT = 'auto'  # 'auto' | 'csv' | 'excel' | 'json'\n",
    "COLUMN_NAME_TO_COMPARE = '파일명'  # column in the input table to match against image filename prefixes\n",
    "\n",
    "# Excel conversion and streaming\n",
    "CONVERT_EXCEL_TO_CSV = True  # if True and input is Excel, convert to CSV for streaming\n",
    "EXCEL_ROWS_THRESHOLD = 15000  # if Excel rows exceed this, convert to CSV for streaming\n",
    "CSV_DELIMITER = ','  # delimiter for CSV reading/writing\n",
    "CSV_ENCODING = 'utf-8-sig'\n",
    "CSV_OUTPUT_ENCODING = 'utf-8-sig'  # used for output CSV to preserve non-ASCII (e.g., Korean) in Excel\n",
    "BATCH_SIZE = 8000  # streaming batch size (mirrors usage in the reference notebook)\n",
    "\n",
    "# Image folder and matching\n",
    "IMAGE_PATH = Path('D:/real_data_key/images_test_key')  # folder containing images (tif, tiff, jpeg, jpg, png, etc.)\n",
    "ALLOWED_IMAGE_EXTENSIONS = {'.tif', '.tiff', '.jpeg', '.jpg', '.png'}\n",
    "MATCH_PREFIX_LEN = 17  # N in filename[:N]\n",
    "STRIP_WHITESPACE = True\n",
    "CASE_INSENSITIVE = True\n",
    "\n",
    "# Output\n",
    "OUTPUT_FOLDER = Path('D:/real_data_key')\n",
    "OUTPUT_FILENAME = 'filtered_rows.xlsx'\n",
    "\n",
    "# JSON nuances (only used if INPUT_FORMAT == 'json' or auto-detected)\n",
    "JSON_RECORDS_IS_LINES = False  # True if NDJSON (one JSON object per line)\n",
    "JSON_ARRAY_FIELD = None        # If the JSON file is an object with an array field, set the field name\n",
    "\n",
    "# Logging verbosity override (optional)\n",
    "DEBUG_LOG = False\n",
    "if DEBUG_LOG:\n",
    "    LOGGER.setLevel(logging.DEBUG)\n",
    "\n",
    "OUTPUT_FOLDER.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65452309",
   "metadata": {},
   "source": [
    "## Helpers: image prefix collection and input streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7ce5a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_image_prefixes(image_dir: Path, allowed_exts: Sequence[str], n: int, case_insensitive: bool) -> List[str]:\n",
    "    image_dir = Path(image_dir)\n",
    "    if not image_dir.exists():\n",
    "        LOGGER.warning(f'IMAGE_PATH does not exist: {image_dir!s}')\n",
    "        return []\n",
    "    allowed = {e.lower() for e in allowed_exts}\n",
    "    prefixes: List[str] = []\n",
    "    for root, _dirs, files in os.walk(image_dir):\n",
    "        for fn in files:\n",
    "            ext = os.path.splitext(fn)[1].lower()\n",
    "            if ext in allowed:\n",
    "                stem = os.path.splitext(fn)[0]\n",
    "                pref = stem[: max(0, int(n))]\n",
    "                if case_insensitive:\n",
    "                    pref = pref.lower()\n",
    "                prefixes.append(pref)\n",
    "    return prefixes\n",
    "\n",
    "def _try_import_openpyxl():\n",
    "    try:\n",
    "        import openpyxl  # type: ignore\n",
    "        return openpyxl\n",
    "    except Exception as e:\n",
    "        LOGGER.warning('openpyxl not available; Excel conversion/streaming may be limited. %s', e)\n",
    "        return None\n",
    "\n",
    "def count_excel_rows(xlsx_path: Path) -> int:\n",
    "    op = _try_import_openpyxl()\n",
    "    if not op:\n",
    "        return -1\n",
    "    wb = op.load_workbook(filename=str(xlsx_path), read_only=True, data_only=True)\n",
    "    ws = wb.active\n",
    "    # max_row includes header\n",
    "    return int(ws.max_row or 0)\n",
    "\n",
    "def excel_to_csv_stream(xlsx_path: Path, csv_out_path: Path, encoding: str = 'utf-8', delimiter: str = ',') -> Path:\n",
    "    op = _try_import_openpyxl()\n",
    "    if not op:\n",
    "        raise RuntimeError('openpyxl is required for Excel to CSV conversion')\n",
    "    LOGGER.info(\n",
    "        \"Converting Excel to CSV for streaming: '%s' -> '%s'\", xlsx_path, csv_out_path\n",
    "    )\n",
    "    wb = op.load_workbook(filename=str(xlsx_path), read_only=True, data_only=True)\n",
    "    ws = wb.active\n",
    "    with open(csv_out_path, 'w', newline='', encoding=encoding) as f:\n",
    "        writer = csv.writer(f, delimiter=delimiter)\n",
    "        for i, row in enumerate(ws.iter_rows(values_only=True), 1):\n",
    "            # Convert None to '' for CSV\n",
    "            out = [ '' if (c is None) else c for c in row ]\n",
    "            writer.writerow(out)\n",
    "    return csv_out_path\n",
    "\n",
    "def read_csv_header(csv_path: Path, delimiter: str = ',') -> List[str]:\n",
    "    with open(csv_path, 'r', newline='', encoding=CSV_ENCODING) as f:\n",
    "        reader = csv.reader(f, delimiter=delimiter)\n",
    "        header = next(reader, None)\n",
    "        return header or []\n",
    "\n",
    "def iter_csv_rows(csv_path: Path, delimiter: str = ',') -> Iterator[Dict[str, Any]]:\n",
    "    with open(csv_path, 'r', newline='', encoding=CSV_ENCODING) as f:\n",
    "        reader = csv.DictReader(f, delimiter=delimiter)\n",
    "        for row in reader:\n",
    "            yield row\n",
    "\n",
    "def iter_json_rows(json_path: Path, records_is_lines: bool = False, array_field: Optional[str] = None) -> Iterator[Dict[str, Any]]:\n",
    "    if records_is_lines:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                yield json.loads(line)\n",
    "    else:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        if isinstance(data, list):\n",
    "            for obj in data:\n",
    "                if isinstance(obj, dict):\n",
    "                    yield obj\n",
    "        elif isinstance(data, dict) and array_field and isinstance(data.get(array_field), list):\n",
    "            for obj in data[array_field]:\n",
    "                if isinstance(obj, dict):\n",
    "                    yield obj\n",
    "        else:\n",
    "            LOGGER.warning('JSON structure not recognized for streaming; expect list or object with array_field')\n",
    "            return\n",
    "\n",
    "def detect_format(path: Path, configured: str = 'auto') -> str:\n",
    "    if configured and configured.lower() != 'auto':\n",
    "        return configured.lower()\n",
    "    ext = path.suffix.lower()\n",
    "    if ext in {'.csv'}:\n",
    "        return 'csv'\n",
    "    if ext in {'.xlsx', '.xls'}:\n",
    "        return 'excel'\n",
    "    if ext in {'.json', '.ndjson'}:\n",
    "        return 'json'\n",
    "    return 'csv'  # default\n",
    "\n",
    "def normalize_value(val: Any, strip_ws: bool, lower: bool) -> str:\n",
    "    s = '' if val is None else str(val)\n",
    "    if strip_ws:\n",
    "        s = s.strip()\n",
    "    if lower:\n",
    "        s = s.lower()\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87800ef7",
   "metadata": {},
   "source": [
    "## Build image prefixes set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d34ee18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 15:17:09,670 | WARNING | filter_key_val_data | IMAGE_PATH does not exist: D:\\real_data_key\\images_test_key\n",
      "2025-09-02 15:17:09,671 | INFO | filter_key_val_data | Collected 0 image filename prefixes from D:\\real_data_key\\images_test_key\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_prefixes = list_image_prefixes(IMAGE_PATH, ALLOWED_IMAGE_EXTENSIONS, MATCH_PREFIX_LEN, CASE_INSENSITIVE)\n",
    "image_prefix_set = set(image_prefixes)\n",
    "LOGGER.info('Collected %d image filename prefixes from %s', len(image_prefix_set), IMAGE_PATH)\n",
    "# Optional preview\n",
    "preview = list(image_prefixes[:10])\n",
    "preview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b619a5b5",
   "metadata": {},
   "source": [
    "## Prepare input streaming (CSV / Excel / JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32fe2b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 15:17:10,494 | INFO | filter_key_val_data | Detected/selected input format: excel\n",
      "2025-09-02 15:17:10,508 | INFO | filter_key_val_data | Converting Excel to CSV for streaming: 'D:\\mock_api_TEST\\filtered_rows.xlsx' -> 'C:\\Users\\jeeb\\AppData\\Local\\Temp\\filtered_rows_key_stream.csv'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmt = detect_format(INPUT_TABLE_PATH, INPUT_FORMAT)\n",
    "LOGGER.info('Detected/selected input format: %s', fmt)\n",
    "\n",
    "tmp_csv_path: Optional[Path] = None\n",
    "header: Optional[List[str]] = None\n",
    "row_iter: Optional[Iterator[Dict[str, Any]]] = None\n",
    "\n",
    "if fmt == 'csv':\n",
    "    header = read_csv_header(INPUT_TABLE_PATH, delimiter=CSV_DELIMITER)\n",
    "    row_iter = iter_csv_rows(INPUT_TABLE_PATH, delimiter=CSV_DELIMITER)\n",
    "elif fmt == 'excel':\n",
    "    do_convert = CONVERT_EXCEL_TO_CSV\n",
    "    if do_convert:\n",
    "        try:\n",
    "            nrows = count_excel_rows(INPUT_TABLE_PATH)\n",
    "        except Exception:\n",
    "            nrows = -1\n",
    "        if (nrows < 0) or (nrows >= EXCEL_ROWS_THRESHOLD):\n",
    "            do_convert = True\n",
    "        else:\n",
    "            # small excel: still convert to unify pipeline (optional)\n",
    "            do_convert = True\n",
    "    if do_convert:\n",
    "        tmp_csv_path = Path(tempfile.gettempdir()) / f\"{INPUT_TABLE_PATH.stem}_key_stream.csv\"\n",
    "        excel_to_csv_stream(INPUT_TABLE_PATH, tmp_csv_path, encoding=CSV_ENCODING, delimiter=CSV_DELIMITER)\n",
    "        header = read_csv_header(tmp_csv_path, delimiter=CSV_DELIMITER)\n",
    "        row_iter = iter_csv_rows(tmp_csv_path, delimiter=CSV_DELIMITER)\n",
    "    else:\n",
    "        # Fallback (not used if we always convert)\n",
    "        raise RuntimeError('Excel direct streaming not configured. Enable CONVERT_EXCEL_TO_CSV.')\n",
    "elif fmt == 'json':\n",
    "    row_iter = iter_json_rows(INPUT_TABLE_PATH, records_is_lines=JSON_RECORDS_IS_LINES, array_field=JSON_ARRAY_FIELD)\n",
    "    header = None  # will infer from the first matching row\n",
    "else:\n",
    "    raise ValueError(f'Unsupported format: {fmt}')\n",
    "\n",
    "row_iter is not None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebf8290",
   "metadata": {},
   "source": [
    "## Stream rows in batches, filter by image prefixes, and write output CSV/XLSX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b3b1ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batches_processed': 1,\n",
       " 'rows_scanned': 26,\n",
       " 'rows_matched': 0,\n",
       " 'output_csv': 'D:\\\\real_data_key\\\\filtered_rows.xlsx',\n",
       " 'input_path': 'D:\\\\mock_api_TEST\\\\filtered_rows.xlsx',\n",
       " 'image_path': 'D:\\\\real_data_key\\\\images_test_key',\n",
       " 'prefix_len': 17}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path = OUTPUT_FOLDER / OUTPUT_FILENAME\n",
    "output_ext = output_path.suffix.lower()\n",
    "written_header = False\n",
    "matched_count = 0\n",
    "scanned_count = 0\n",
    "batches_processed = 0\n",
    "\n",
    "# Prepare output handles lazily when we see first match (header may need to be inferred)\n",
    "out_fh = None\n",
    "writer = None\n",
    "wb = None\n",
    "ws = None\n",
    "\n",
    "def ensure_outputs(fieldnames: List[str]):\n",
    "    global writer, out_fh, wb, ws, written_header\n",
    "    if (writer is None) and (wb is None):\n",
    "        if output_ext == '.xlsx':\n",
    "            op = _try_import_openpyxl()\n",
    "            if not op:\n",
    "                raise RuntimeError('openpyxl is required to write XLSX output')\n",
    "            wb = op.Workbook()\n",
    "            ws = wb.active\n",
    "            ws.title = 'filtered'\n",
    "            ws.append(fieldnames)\n",
    "        else:\n",
    "            mode = 'w'\n",
    "            out_fh = open(output_path, mode, newline='', encoding=CSV_OUTPUT_ENCODING)\n",
    "            writer = csv.DictWriter(out_fh, fieldnames=fieldnames, delimiter=CSV_DELIMITER)\n",
    "            writer.writeheader()\n",
    "\n",
    "def write_rows(rows: List[Dict[str, Any]], fieldnames: List[str]):\n",
    "    if ws is not None:\n",
    "        for row in rows:\n",
    "            ws.append([row.get(col) for col in fieldnames])\n",
    "    else:\n",
    "        for row in rows:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Build a streaming of the normalized compare values paired with full row (dict)\n",
    "def normalized_rows(iter_rows: Iterator[Dict[str, Any]]) -> Iterator[Tuple[str, Dict[str, Any]]]:\n",
    "    for r in iter_rows:\n",
    "        # Normalize value (strip whitespace and case-fold if CASE_INSENSITIVE), then align to prefix length\n",
    "        key_val = normalize_value(r.get(COLUMN_NAME_TO_COMPARE), STRIP_WHITESPACE, CASE_INSENSITIVE)\n",
    "        if MATCH_PREFIX_LEN is not None:\n",
    "            try:\n",
    "                key_val = key_val[: max(0, int(MATCH_PREFIX_LEN))]\n",
    "            except Exception:\n",
    "                key_val = key_val\n",
    "        yield key_val, r\n",
    "\n",
    "for i, batch in enumerate(_ensure_iterable_chunks(normalized_rows(row_iter), BATCH_SIZE), 1):\n",
    "    batches_processed += 1\n",
    "    scanned_count += len(batch)\n",
    "    # Filter matches\n",
    "    matches: List[Dict[str, Any]] = []\n",
    "    for key_val, row in batch:\n",
    "        if key_val in image_prefix_set:\n",
    "            matches.append(row)\n",
    "    if not matches:\n",
    "        continue\n",
    "    matched_count += len(matches)\n",
    "    # Initialize outputs if needed\n",
    "    if header is None:\n",
    "        # infer from first match keys\n",
    "        header = list(matches[0].keys())\n",
    "    ensure_outputs(header)\n",
    "    # Write matches\n",
    "    write_rows(matches, header)\n",
    "    LOGGER.info('Batch %d: scanned=%d, matched=%d (cumulative matched=%d)', i, len(batch), len(matches), matched_count)\n",
    "\n",
    "# Finalize outputs\n",
    "if writer is not None and out_fh is not None:\n",
    "    out_fh.close()\n",
    "if wb is not None:\n",
    "    wb.save(str(output_path))\n",
    "\n",
    "summary = {\n",
    "    'batches_processed': batches_processed,\n",
    "    'rows_scanned': scanned_count,\n",
    "    'rows_matched': matched_count,\n",
    "    'output_csv': str(output_path),\n",
    "    'input_path': str(INPUT_TABLE_PATH),\n",
    "    'image_path': str(IMAGE_PATH),\n",
    "    'prefix_len': MATCH_PREFIX_LEN,\n",
    "}\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987a30b2",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- If your input is Excel and very large, ensure `openpyxl` is installed for efficient streaming conversion. The notebook will convert Excel to a temporary CSV to stream rows in batches.\n",
    "- JSON inputs: set `JSON_RECORDS_IS_LINES=True` for NDJSON (one JSON object per line) or `JSON_ARRAY_FIELD` when the JSON file is an object with an embedded array.\n",
    "- Matching is exact on the normalized values: row[COLUMN_NAME_TO_COMPARE] (optionally stripped and lowercased) must equal the image filename prefix (filename[:MATCH_PREFIX_LEN]) built in the same normalization mode.\n",
    "- Output header for CSV/XLSX is derived from the input CSV header when applicable, otherwise inferred from the first matched row for JSON."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image-similarity-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
